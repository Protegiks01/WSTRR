# Audit Report

## Title
Coordinator Panic Due to Premature Wait List Removal Before Signature Share Validation

## Summary
The `gather_sig_shares()` function in FireCoordinator removes signers from the wait list before validating their signature share responses. When validation subsequently fails, the signer remains in `public_nonces` but is absent from `signature_shares`, causing a panic during aggregation when the code attempts to index into the BTreeMap. A single malicious signer within the threshold can exploit this to crash the coordinator node.

## Finding Description

The vulnerability exists in the FireCoordinator's `gather_sig_shares()` function where the ordering of operations creates a critical invariant violation between two data structures.

During nonce gathering, signers are added to both `public_nonces` and `sign_wait_signer_ids`: [1](#0-0) 

Later, during signature share gathering, the coordinator removes a signer from the wait list immediately upon receiving their response, before validating it: [2](#0-1) 

However, critical validation checks occur AFTER this removal: [3](#0-2) 

These validations can fail and return errors (e.g., missing public key at lines 1050-1052, missing key IDs at line 1063, or mismatched key_ids at line 1075). When validation fails, the function returns an error without adding the signer to `signature_shares`: [4](#0-3) 

The signer was already removed from the wait list but was never added to `signature_shares`. Meanwhile, the signer's entry remains in `public_nonces` from the earlier nonce gathering phase.

When all other honest signers respond successfully, the wait list becomes empty and aggregation proceeds: [5](#0-4) 

At line 1134, the code iterates over all signers in `public_nonces` and attempts to access their shares via BTreeMap indexing using `self.signature_shares[i]`. Since `signature_shares` is a BTreeMap<u32, Vec<SignatureShare>> and the failed signer's ID is not present, Rust's Index trait panics with "no entry found for key".

The error handler in `process_message()` only catches returned errors, not panics: [6](#0-5) 

**Attack Scenario:**
1. Malicious signer participates normally in DKG and nonce gathering (gets added to `public_nonces`)
2. During signature share gathering, malicious signer sends a SignatureShareResponse with incorrect `key_ids` that don't match their configured keys
3. The coordinator removes the signer from the wait list (line 1042-1044)
4. Validation fails at line 1073-1076, returning `Error::BadKeyIDsForSigner`
5. The error is caught and converted to OperationResult::SignError (line 328-333)
6. The signer remains in `public_nonces` but is NOT in `signature_shares`
7. Other honest signers respond successfully, emptying the wait list
8. When the last honest signer's response is processed, aggregation begins
9. At line 1134, iteration over `public_nonces` attempts to access `self.signature_shares[malicious_signer_id]`
10. BTreeMap Index panics, crashing the coordinator

## Impact Explanation

This vulnerability has **Low** severity according to the WSTS scope definition: "Any remotely-exploitable denial of service in a node."

The panic causes:
- **Coordinator Node Crash**: The panic terminates the coordinator process with no graceful error handling
- **Transaction Signing Failure**: The current signing round fails, requiring coordinator restart
- **Repeatable DoS**: The malicious signer is not marked as malicious, allowing repeated attacks
- **No Automatic Recovery**: Manual intervention is required to restart the coordinator

While the report claims "Critical" severity based on potential network shutdown, this assessment requires speculative assumptions about WSTS deployment architecture. The vulnerability itself causes a single coordinator node DoS, which maps to Low severity per the defined scope.

## Likelihood Explanation

This vulnerability has **High** likelihood:

**Attacker Capabilities Required:**
- Control of a single signer within the threshold set (explicitly within threat model)
- Ability to send protocol messages to the coordinator (standard operation)
- No cryptographic breaks or secrets required

**Attack Complexity:**
- **Low**: The attacker sends a SignatureShareResponse with mismatched `key_ids` field
- No race conditions or timing requirements needed
- Deterministic outcome (100% success rate)
- Attack can be repeated on every signing round

**Detection Difficulty:**
- **High**: The panic occurs during aggregation when processing a different (honest) signer's message, not during the malicious signer's message processing
- No attribution logging before the panic
- The malicious signer appears to have sent a "valid" message (it passed initial checks)

## Recommendation

Fix the ordering of operations in `gather_sig_shares()` to validate before modifying state:

```rust
// Validate first, then remove from wait list
// Move validation checks (lines 1046-1076) BEFORE wait list removal (lines 1042-1044)

// Only after ALL validations pass:
response_info
    .sign_wait_signer_ids
    .remove(&sig_share_response.signer_id);

self.signature_shares.insert(
    sig_share_response.signer_id,
    sig_share_response.signature_shares.clone(),
);
```

Additionally, use safe BTreeMap access during aggregation:

```rust
// Replace line 1134 indexing with safe access:
let shares = message_nonce
    .public_nonces
    .iter()
    .filter_map(|(i, _)| self.signature_shares.get(i).cloned())
    .flatten()
    .collect::<Vec<SignatureShare>>();
```

This ensures that only signers present in both `public_nonces` AND `signature_shares` contribute to aggregation.

## Proof of Concept

```rust
#[test]
fn test_coordinator_panic_on_premature_wait_list_removal() {
    // Setup: Create FireCoordinator with 3 signers, threshold 2
    let mut coordinator = setup_coordinator_with_signers(3, 2);
    
    // Step 1: Complete DKG successfully
    complete_dkg_round(&mut coordinator);
    
    // Step 2: Start signing round, all signers send valid nonces
    let message = b"test message";
    coordinator.start_signing_round(message, SignatureType::Frost, None);
    send_nonces_from_all_signers(&mut coordinator, message);
    
    // Step 3: Signer 0 sends SignatureShareResponse with MISMATCHED key_ids
    let mut bad_response = create_signature_share_response(0);
    bad_response.signature_shares[0].key_ids = vec![999]; // Invalid key_id
    let bad_packet = create_packet(bad_response);
    coordinator.process_message(&bad_packet); // Returns error, signer removed from wait list
    
    // Step 4: Signers 1 and 2 send valid SignatureShareResponses
    let good_response_1 = create_valid_signature_share_response(1, message);
    coordinator.process_message(&create_packet(good_response_1)); // OK
    
    let good_response_2 = create_valid_signature_share_response(2, message);
    // This should panic when trying to aggregate because signer 0 is in
    // public_nonces but not in signature_shares
    coordinator.process_message(&create_packet(good_response_2)); // PANIC!
}
```

The test demonstrates that when signer 0's validation fails after wait list removal, subsequent processing of valid responses triggers a panic during aggregation.

## Notes

This vulnerability affects both FireCoordinator and potentially FrostCoordinator implementations. The root cause is the violation of the invariant that all signers in `public_nonces` must have corresponding entries in `signature_shares` before aggregation. The premature state modification (wait list removal) before validation creates this inconsistency.

### Citations

**File:** src/state_machine/coordinator/fire.rs (L327-333)
```rust
                State::SigShareGather(signature_type) => {
                    if let Err(e) = self.gather_sig_shares(packet, signature_type) {
                        return Ok((
                            None,
                            Some(OperationResult::SignError(SignError::Coordinator(e))),
                        ));
                    }
```

**File:** src/state_machine/coordinator/fire.rs (L931-942)
```rust
            nonce_info
                .public_nonces
                .insert(nonce_response.signer_id, nonce_response.clone());

            // ignore the passed key_ids
            for key_id in signer_key_ids {
                nonce_info.nonce_recv_key_ids.insert(*key_id);
            }

            nonce_info
                .sign_wait_signer_ids
                .insert(nonce_response.signer_id);
```

**File:** src/state_machine/coordinator/fire.rs (L1040-1044)
```rust
        // we were waiting on you, and you sent a packet for this sign round, so we won't take
        // another packet from you
        response_info
            .sign_wait_signer_ids
            .remove(&sig_share_response.signer_id);
```

**File:** src/state_machine/coordinator/fire.rs (L1046-1076)
```rust
        // check that the signer_id exists in the config
        let signer_public_keys = &self.config.public_keys.signers;
        if !signer_public_keys.contains_key(&sig_share_response.signer_id) {
            warn!(signer_id = %sig_share_response.signer_id, "No public key in config");
            return Err(Error::MissingPublicKeyForSigner(
                sig_share_response.signer_id,
            ));
        };

        // check that the key_ids match the config
        let Some(signer_key_ids) = self
            .config
            .public_keys
            .signer_key_ids
            .get(&sig_share_response.signer_id)
        else {
            warn!(signer_id = %sig_share_response.signer_id, "No keys IDs configured");
            return Err(Error::MissingKeyIDsForSigner(sig_share_response.signer_id));
        };

        let mut sig_share_response_key_ids = HashSet::new();
        for sig_share in &sig_share_response.signature_shares {
            for key_id in &sig_share.key_ids {
                sig_share_response_key_ids.insert(*key_id);
            }
        }

        if *signer_key_ids != sig_share_response_key_ids {
            warn!(signer_id = %sig_share_response.signer_id, "SignatureShareResponse key_ids didn't match config");
            return Err(Error::BadKeyIDsForSigner(sig_share_response.signer_id));
        }
```

**File:** src/state_machine/coordinator/fire.rs (L1088-1091)
```rust
        self.signature_shares.insert(
            sig_share_response.signer_id,
            sig_share_response.signature_shares.clone(),
        );
```

**File:** src/state_machine/coordinator/fire.rs (L1113-1134)
```rust
        if message_nonce.sign_wait_signer_ids.is_empty() {
            // Calculate the aggregate signature
            let nonce_responses = message_nonce
                .public_nonces
                .values()
                .cloned()
                .collect::<Vec<NonceResponse>>();

            let nonces = nonce_responses
                .iter()
                .flat_map(|nr| nr.nonces.clone())
                .collect::<Vec<PublicNonce>>();

            let key_ids = nonce_responses
                .iter()
                .flat_map(|nr| nr.key_ids.clone())
                .collect::<Vec<u32>>();

            let shares = message_nonce
                .public_nonces
                .iter()
                .flat_map(|(i, _)| self.signature_shares[i].clone())
```
