# Audit Report

## Title
Coordinator Panic Due to Premature Wait List Removal in Signature Share Gathering

## Summary
The FIRE coordinator removes signers from the wait list before validating their signature share responses in the `gather_sig_shares` function. When validation subsequently fails, the signer is removed from the wait list but never added to `signature_shares`, creating a state inconsistency. During aggregation, the coordinator attempts to access the missing signer's shares using bracket notation on a BTreeMap, causing a panic that crashes the coordinator process.

## Finding Description

The vulnerability exists in the `gather_sig_shares` function where the coordinator processes `SignatureShareResponse` messages. The critical flaw is the order of operations:

**1. Premature Wait List Removal**: The coordinator removes the signer from `sign_wait_signer_ids` immediately after confirming they are in the wait list, but BEFORE performing any validation checks. [1](#0-0) 

**2. Post-Removal Validation**: Multiple validation checks occur AFTER the wait list removal, any of which can fail and return errors:
- Missing public key check (lines 1048-1053)
- Missing key IDs check (lines 1056-1064)  
- Mismatched key IDs check (lines 1073-1076) [2](#0-1) 

**3. Conditional Insertion**: The signature shares are only inserted into `signature_shares` if ALL validations pass. [3](#0-2) 

**4. Corrupted State Persistence**: When validation fails, the error is returned and caught as an `OperationResult::SignError`, but the coordinator state remains in `SigShareGather` with the signer already removed from the wait list but absent from `signature_shares`. [4](#0-3) 

**5. Unsafe Aggregation Access**: When the wait list becomes empty (after other signers successfully complete), aggregation iterates over ALL signers in `public_nonces` and accesses `signature_shares[i]` using bracket notation. Since `signature_shares` is a BTreeMap, this panics when the key doesn't exist. [5](#0-4) 

**Root Cause**: The code assumes `public_nonces.keys()` equals `signature_shares.keys()` during aggregation, but the premature wait list removal breaks this invariant.

**Attack Scenario:**
1. Attacker (registered signer) participates in nonce gathering with valid nonces → added to `public_nonces`
2. Attacker sends `SignatureShareResponse` with malformed data (e.g., key_ids that don't match config)
3. Coordinator removes attacker from wait list (lines 1042-1044)
4. Validation fails (e.g., at lines 1073-1076), function returns error
5. Attacker NOT added to `signature_shares`, but already removed from wait list
6. Other honest signers send valid responses and complete successfully
7. Wait list becomes empty, triggering aggregation (line 1113)
8. Line 1134 tries to access `signature_shares[attacker_id]` → **panic!**

**Confirmation via FROST Comparison**: The FROST coordinator does NOT have this vulnerability because it performs the wait list removal AFTER validation and insertion: [6](#0-5) 

This confirms the FIRE implementation has a genuine ordering bug that breaks the state machine invariant.

## Impact Explanation

This vulnerability allows a single malicious signer to crash the coordinator node through a Rust panic, resulting in process termination. The impact maps to **Low severity** under the scope definition: "Any remotely-exploitable denial of service in a node."

The coordinator process must be restarted to resume operations, and the affected signing round must be retried. While disruptive to availability, this does not:
- Cause direct loss of funds
- Enable invalid signature acceptance  
- Trigger consensus failures
- Create persistent state corruption

Applications using WSTS FIRE coordinator for threshold signature coordination (such as Stacks blockchain signers) would experience service interruption until the coordinator is restarted. However, the coordinator can recover by simply restarting the process and initiating a new signing round.

## Likelihood Explanation

**Likelihood: High (~100% success rate)**

The attack is deterministic and trivial to execute:

**Required Capabilities:**
- Attacker must be a registered signer in the WSTS configuration (within protocol threat model of up to threshold-1 malicious signers)
- Must participate in nonce gathering phase with valid nonces
- Must send a malformed `SignatureShareResponse`

**Attack Complexity:** Very low
- No cryptographic operations required
- No timing dependencies or race conditions
- Single malformed packet triggers the vulnerability
- Example trigger: Send signature shares with key_ids that don't match the configured signer_key_ids

**Economic Cost:** Negligible
- No computational expense beyond normal protocol participation
- No need to control multiple signers

**Detection Difficulty:** Low
- Appears as a validation error followed by an unexpected crash
- Difficult to distinguish from legitimate software bugs without detailed forensics

**Success Probability:** ~100% deterministic - the panic will occur reliably once the conditions are met

## Recommendation

The fix is to reorder operations to match the FROST implementation: perform wait list removal AFTER successful validation and insertion into `signature_shares`.

**Corrected order of operations:**
1. Check if signer is in wait list
2. Perform all validations (public key, key IDs, etc.)
3. Insert signature shares into `signature_shares`
4. Remove signer from wait list

This ensures the invariant `public_nonces.keys() ⊇ signature_shares.keys()` is always maintained, preventing the panic during aggregation. The wait list removal should be the LAST operation after all validations pass and data is successfully stored.

## Proof of Concept

```rust
// Conceptual PoC demonstrating the vulnerability flow
#[test]
#[should_panic(expected = "no entry found for key")]
fn test_premature_wait_list_removal_panic() {
    // Setup: Create coordinator with 3 signers, threshold 2
    let (mut coordinator, signers) = setup_fire_coordinator(3, 1);
    
    // Step 1: Complete DKG to get aggregate public key
    run_dkg(&mut coordinator, &signers);
    
    // Step 2: Start signing round
    let message = b"test message".to_vec();
    coordinator.start_signing_round(&message, SignatureType::Schnorr, None);
    
    // Step 3: All signers (including attacker) send valid nonces
    for signer in &signers {
        let nonce_response = signer.generate_nonce_response();
        coordinator.process_message(&nonce_response);
    }
    
    // Step 4: Coordinator requests signature shares
    coordinator.request_sig_shares(SignatureType::Schnorr);
    
    // Step 5: Attacker (signer 0) sends malformed SignatureShareResponse
    // with wrong key_ids to trigger validation failure
    let malformed_response = create_malformed_sig_share_response(
        signers[0].id,
        wrong_key_ids  // This will fail validation at line 1073-1076
    );
    
    // This removes signer from wait list BUT validation fails
    // Signer NOT added to signature_shares
    coordinator.gather_sig_shares(&malformed_response, SignatureType::Schnorr);
    
    // Step 6: Other honest signers send valid responses
    for signer in &signers[1..] {
        let valid_response = signer.generate_sig_share_response();
        coordinator.gather_sig_shares(&valid_response, SignatureType::Schnorr);
    }
    
    // Step 7: Wait list is now empty, triggers aggregation
    // Line 1134: tries to access signature_shares[signer_0_id]
    // PANIC! Key doesn't exist in BTreeMap
}
```

The test demonstrates that when a signer's signature share response fails validation after wait list removal, the subsequent aggregation step will panic when attempting to access the missing entry in `signature_shares`.

### Citations

**File:** src/state_machine/coordinator/fire.rs (L327-333)
```rust
                State::SigShareGather(signature_type) => {
                    if let Err(e) = self.gather_sig_shares(packet, signature_type) {
                        return Ok((
                            None,
                            Some(OperationResult::SignError(SignError::Coordinator(e))),
                        ));
                    }
```

**File:** src/state_machine/coordinator/fire.rs (L1015-1044)
```rust
        let waiting = response_info
            .sign_wait_signer_ids
            .contains(&sig_share_response.signer_id);

        if !waiting {
            warn!(
                "Sign round {} SignatureShareResponse for round {} from signer {} not in the wait list",
                self.current_sign_id, sig_share_response.sign_id, sig_share_response.signer_id,
            );
            return Ok(());
        }

        if sig_share_response.dkg_id != self.current_dkg_id {
            return Err(Error::BadDkgId(
                sig_share_response.dkg_id,
                self.current_dkg_id,
            ));
        }
        if sig_share_response.sign_id != self.current_sign_id {
            return Err(Error::BadSignId(
                sig_share_response.sign_id,
                self.current_sign_id,
            ));
        }

        // we were waiting on you, and you sent a packet for this sign round, so we won't take
        // another packet from you
        response_info
            .sign_wait_signer_ids
            .remove(&sig_share_response.signer_id);
```

**File:** src/state_machine/coordinator/fire.rs (L1046-1076)
```rust
        // check that the signer_id exists in the config
        let signer_public_keys = &self.config.public_keys.signers;
        if !signer_public_keys.contains_key(&sig_share_response.signer_id) {
            warn!(signer_id = %sig_share_response.signer_id, "No public key in config");
            return Err(Error::MissingPublicKeyForSigner(
                sig_share_response.signer_id,
            ));
        };

        // check that the key_ids match the config
        let Some(signer_key_ids) = self
            .config
            .public_keys
            .signer_key_ids
            .get(&sig_share_response.signer_id)
        else {
            warn!(signer_id = %sig_share_response.signer_id, "No keys IDs configured");
            return Err(Error::MissingKeyIDsForSigner(sig_share_response.signer_id));
        };

        let mut sig_share_response_key_ids = HashSet::new();
        for sig_share in &sig_share_response.signature_shares {
            for key_id in &sig_share.key_ids {
                sig_share_response_key_ids.insert(*key_id);
            }
        }

        if *signer_key_ids != sig_share_response_key_ids {
            warn!(signer_id = %sig_share_response.signer_id, "SignatureShareResponse key_ids didn't match config");
            return Err(Error::BadKeyIDsForSigner(sig_share_response.signer_id));
        }
```

**File:** src/state_machine/coordinator/fire.rs (L1078-1091)
```rust
        let have_shares = self
            .signature_shares
            .contains_key(&sig_share_response.signer_id);

        if have_shares {
            info!(signer_id = %sig_share_response.signer_id, "received duplicate SignatureShareResponse");
            // XXX should this be an error?  We should have already removed signer from wait set
            return Ok(());
        }

        self.signature_shares.insert(
            sig_share_response.signer_id,
            sig_share_response.signature_shares.clone(),
        );
```

**File:** src/state_machine/coordinator/fire.rs (L1131-1135)
```rust
            let shares = message_nonce
                .public_nonces
                .iter()
                .flat_map(|(i, _)| self.signature_shares[i].clone())
                .collect::<Vec<SignatureShare>>();
```

**File:** src/state_machine/coordinator/frost.rs (L643-656)
```rust
            let have_shares = self
                .signature_shares
                .contains_key(&sig_share_response.signer_id);

            if have_shares {
                info!(signer_id = %sig_share_response.signer_id, "received duplicate SignatureShareResponse");
                return Ok(());
            }

            self.signature_shares.insert(
                sig_share_response.signer_id,
                sig_share_response.signature_shares.clone(),
            );
            self.ids_to_await.remove(&sig_share_response.signer_id);
```
