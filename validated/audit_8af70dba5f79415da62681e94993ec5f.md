# Audit Report

## Title
Coordinator Assumes Message Ordering Without Validation in v1 Aggregation

## Summary
The v1 coordinator collects nonces and signature shares from signers by flat-mapping their responses, assuming positional correspondence. However, it only validates SET equality of key_ids using HashSet comparison, not ORDER. A malicious v1 signer controlling multiple parties can send responses with mismatched internal ordering, causing the aggregator to pair wrong nonces with wrong parties during signature verification, resulting in denial of service.

## Finding Description
The vulnerability exists in the v1 signing protocol where each signer controls multiple `Party` objects, each generating its own nonce and signature share. [1](#0-0) 

**Attack Execution Path:**

1. **Nonce Collection**: The coordinator flat-maps nonces from NonceResponse messages: [2](#0-1) 

2. **Signature Share Collection**: The coordinator flat-maps signature shares: [3](#0-2) 

3. **Validation Gap**: The coordinator validates only SET equality of key_ids using HashSet, not ordering: [4](#0-3) 

4. **Aggregator Assumption**: The v1 aggregator extracts party IDs from signature shares positionally: [5](#0-4) 

5. **Positional Pairing**: `compute::intermediate` zips party_ids with nonces by position: [6](#0-5) 

**The Exploit:**

A malicious v1 signer with parties [1,2,3] can:
- Generate nonces normally via `gen_nonces`: [7](#0-6) 
- Send NonceResponse with key_ids=[1,2,3], nonces=[N1,N2,N3]
- Send SignatureShareResponse with signature_shares reordered: [share3, share2, share1] where share_i.id=i
- Coordinator's HashSet validation passes (set equality)
- Aggregator extracts party_ids=[3,2,1] from reordered shares
- `compute::intermediate` zips: [(N1,rho3), (N2,rho2), (N3,rho1)]
- Incorrect pairing causes verification failure

**Why v1-Specific:**

v1 signers generate multiple nonces (one per party): [7](#0-6) 

v2 signers generate only ONE nonce regardless of key_ids: [8](#0-7) 

## Impact Explanation
This vulnerability enables denial of service against the signing protocol. When signature verification fails due to incorrect nonce pairing, the coordinator cannot produce valid signatures: [9](#0-8) 

**Impact Classification: Medium** - This causes transient consensus failures as defined in scope. All signature attempts fail when the malicious v1 signer participates, preventing block production or transaction signing until the malicious signer is identified and removed. This does not cause permanent fund loss or persistent consensus failures, but temporarily prevents valid signature creation.

## Likelihood Explanation
**Likelihood: High**

Required capabilities:
- Attacker must be a legitimate v1 signer with valid DKG shares (within threat model)
- Attacker constructs custom SignatureShareResponse with reordered signature_shares vector
- No cryptographic breaks or key compromise required

Attack complexity is LOW:
1. Participate in DKG normally to become legitimate signer
2. During signing, send standard NonceResponse
3. Send SignatureShareResponse with signature_shares reordered (e.g., reverse order)
4. Coordinator's HashSet validation passes but aggregation fails

The attack is trivially executable and within the protocol threat model (malicious signers up to threshold-1).

## Recommendation
The coordinator should enforce strict ordering validation between NonceResponse and SignatureShareResponse:

1. Store the key_ids order from NonceResponse for each signer
2. When validating SignatureShareResponse, verify that signature_shares are ordered such that extracting party IDs yields the same order as the stored key_ids
3. Alternatively, modify the aggregator to explicitly map signature shares to nonces by party ID rather than relying on positional correspondence

Example validation:
```rust
// After receiving NonceResponse, store key_ids order
self.nonce_key_ids_order.insert(signer_id, nonce_response.key_ids.clone());

// When validating SignatureShareResponse
let expected_order = self.nonce_key_ids_order.get(&signer_id)?;
let actual_order: Vec<u32> = sig_share_response.signature_shares
    .iter()
    .map(|ss| ss.id)
    .collect();
if expected_order != &actual_order {
    return Err(Error::MismatchedSignatureShareOrder(signer_id));
}
```

## Proof of Concept
```rust
#[test]
fn test_v1_signature_share_reordering_attack() {
    use crate::v1::{Aggregator, Party, Signer};
    use crate::traits::{Aggregator as AggregatorTrait, Signer as SignerTrait};
    use rand_core::OsRng;
    
    let mut rng = OsRng;
    let threshold = 3;
    let num_keys = 5;
    
    // Create v1 signer with 3 parties
    let mut signer = Signer::new(0, &[1, 2, 3], num_keys, threshold, &mut rng);
    
    // Generate nonces (in order: party 1, 2, 3)
    let nonces = signer.gen_nonces(&Scalar::random(&mut rng), &mut rng);
    assert_eq!(nonces.len(), 3);
    
    // Generate signature shares (in order: party 1, 2, 3)
    let key_ids = vec![1, 2, 3];
    let msg = b"test message";
    let mut shares = signer.sign(msg, &[], &key_ids, &nonces);
    assert_eq!(shares.len(), 3);
    
    // Attacker reorders signature shares: [3, 2, 1]
    shares.reverse();
    
    // Aggregator processes with mismatched ordering
    let mut aggregator = Aggregator::new(num_keys, threshold);
    // aggregator.init(...) would be called here
    
    // This will fail because nonces[i] doesn't match shares[i].id
    // The aggregator extracts party_ids from shares: [3, 2, 1]
    // But nonces are still in original order for parties [1, 2, 3]
    // compute::intermediate zips them: [(nonce_for_1, rho_3), ...]
    let result = aggregator.sign(msg, &nonces, &shares, &key_ids);
    assert!(result.is_err(), "Signature should fail due to nonce mismatch");
}
```

## Notes
This vulnerability is specific to v1 implementation due to its architecture where each signer manages multiple Party objects. The v2 implementation is not affected because each signer generates only a single nonce and signature share, eliminating the possibility of internal reordering attacks. The fix should be applied to v1 coordinator logic to enforce strict ordering validation or modify the aggregation logic to use explicit party ID mapping instead of positional correspondence.

### Citations

**File:** src/v1.rs (L325-326)
```rust
        let signers: Vec<u32> = sig_shares.iter().map(|ss| ss.id).collect();
        let (_Rs, R) = compute::intermediate(msg, &signers, nonces);
```

**File:** src/v1.rs (L399-416)
```rust
        for i in 0..sig_shares.len() {
            let id = compute::id(sig_shares[i].id);
            let public_key = match compute::poly(&id, &self.poly) {
                Ok(p) => p,
                Err(_) => {
                    bad_party_keys.push(sig_shares[i].id);
                    Point::zero()
                }
            };

            let z_i = sig_shares[i].z_i;

            if z_i * G
                != r_sign * Rs[i]
                    + cx_sign * (compute::lambda(sig_shares[i].id, &signers) * c * public_key)
            {
                bad_party_sigs.push(sig_shares[i].id);
            }
```

**File:** src/v1.rs (L514-526)
```rust
/// A set of encapsulated FROST parties
pub struct Signer {
    /// The associated signer ID
    id: u32,
    /// The total number of keys
    num_keys: u32,
    /// The threshold of the keys needed to make a valid signature
    threshold: u32,
    /// The aggregate group public key
    group_key: Point,
    /// The parties which this object encapsulates
    parties: Vec<Party>,
}
```

**File:** src/v1.rs (L676-685)
```rust
    fn gen_nonces<RNG: RngCore + CryptoRng>(
        &mut self,
        secret_key: &Scalar,
        rng: &mut RNG,
    ) -> Vec<PublicNonce> {
        self.parties
            .iter_mut()
            .map(|p| p.gen_nonce(secret_key, rng))
            .collect()
    }
```

**File:** src/state_machine/coordinator/fire.rs (L1066-1076)
```rust
        let mut sig_share_response_key_ids = HashSet::new();
        for sig_share in &sig_share_response.signature_shares {
            for key_id in &sig_share.key_ids {
                sig_share_response_key_ids.insert(*key_id);
            }
        }

        if *signer_key_ids != sig_share_response_key_ids {
            warn!(signer_id = %sig_share_response.signer_id, "SignatureShareResponse key_ids didn't match config");
            return Err(Error::BadKeyIDsForSigner(sig_share_response.signer_id));
        }
```

**File:** src/state_machine/coordinator/fire.rs (L1121-1129)
```rust
            let nonces = nonce_responses
                .iter()
                .flat_map(|nr| nr.nonces.clone())
                .collect::<Vec<PublicNonce>>();

            let key_ids = nonce_responses
                .iter()
                .flat_map(|nr| nr.key_ids.clone())
                .collect::<Vec<u32>>();
```

**File:** src/state_machine/coordinator/fire.rs (L1131-1135)
```rust
            let shares = message_nonce
                .public_nonces
                .iter()
                .flat_map(|(i, _)| self.signature_shares[i].clone())
                .collect::<Vec<SignatureShare>>();
```

**File:** src/compute.rs (L85-96)
```rust
pub fn intermediate(msg: &[u8], party_ids: &[u32], nonces: &[PublicNonce]) -> (Vec<Point>, Point) {
    let rhos: Vec<Scalar> = party_ids
        .iter()
        .map(|&i| binding(&id(i), nonces, msg))
        .collect();
    let R_vec: Vec<Point> = zip(nonces, rhos)
        .map(|(nonce, rho)| nonce.D + rho * nonce.E)
        .collect();

    let R = R_vec.iter().fold(Point::zero(), |R, &R_i| R + R_i);
    (R_vec, R)
}
```

**File:** src/v2.rs (L627-633)
```rust
    fn gen_nonces<RNG: RngCore + CryptoRng>(
        &mut self,
        secret_key: &Scalar,
        rng: &mut RNG,
    ) -> Vec<PublicNonce> {
        vec![self.gen_nonce(secret_key, rng)]
    }
```
