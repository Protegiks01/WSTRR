# Audit Report

## Title
Concurrent Coordinators Cause DKG Consensus Failures via Unprotected Signer State Reset

## Summary
Signers unconditionally reset their DKG state and regenerate random polynomials upon receiving each `DkgBegin` message, without checking if they have already processed that `dkg_id`. When multiple coordinators simultaneously initiate DKG rounds, signers reset multiple times, sending different polynomial commitments in response to each coordinator. Due to network timing, different coordinators collect different sets of commitments from signers, resulting in computation of different aggregate public keys and consensus failure.

## Finding Description

The vulnerability exists in the signer's message processing flow. When a signer receives a `DkgBegin` message, it unconditionally calls `reset()` which regenerates random polynomials, regardless of whether it has already started processing that `dkg_id`. [1](#0-0) 

The `reset()` function clears all DKG state and calls `reset_polys()` to generate new random polynomials: [2](#0-1) 

The `reset_polys()` implementation generates a completely new random polynomial, invalidating any previous commitments: [3](#0-2) 

**Root Cause - Missing Duplicate Detection:**

Unlike signers, coordinators explicitly check if they have already processed a `dkg_id` before proceeding: [4](#0-3) 

Signers lack this critical protection, allowing them to reset state multiple times for the same `dkg_id`.

**Why This Breaks Security:**

Coordinators de-duplicate responses by `signer_id`, accepting only the FIRST `DkgPublicShares` received from each signer: [5](#0-4) 

When signers send multiple different `DkgPublicShares` messages (due to repeated resets), network timing determines which message each coordinator receives first. Different coordinators end up with different sets of polynomial commitments from the same signers, causing them to compute different aggregate public keys.

**Evidence The Test Suite Hides This Issue:**

The test infrastructure only propagates messages from the first coordinator (`i == 0`), preventing concurrent coordinator messages from reaching signers: [6](#0-5) 

This masking indicates concurrent coordinator scenarios were not tested during development.

## Impact Explanation

**Consensus Failure Scenario:**

When two coordinators with valid signing credentials both initiate DKG:
1. Both send `DkgBegin{dkg_id: 1}` to all signers
2. Signer S1 receives Coordinator A's message → resets, generates polynomial P1_A, sends shares
3. Signer S1 receives Coordinator B's message → resets again, generates polynomial P1_B, sends different shares
4. Due to network timing, Coordinator A receives P1_A shares from some signers and P1_B from others
5. Coordinator B receives a different mix of P1_A and P1_B shares
6. Different coordinators compute different aggregate public keys

This violates the fundamental DKG invariant that all participants must derive the same group public key. In blockchain deployments using WSTS (like Stacks), this causes different nodes to compute different signing keys, leading to consensus failures when attempting to validate signatures.

**Severity Justification:**

This maps to **Medium severity: "Any transient consensus failures"** as defined in the scope. Different nodes compute different cryptographic keys, causing consensus divergence that prevents the network from agreeing on valid signatures.

## Likelihood Explanation

**Triggering Conditions:**

This vulnerability can be triggered in multiple realistic scenarios:

1. **Operational Error**: Administrator accidentally launches two coordinator instances with the same configuration file containing the same `message_private_key`
2. **High Availability Race**: Multiple HA replicas both become active during failover
3. **Leader Election Failure**: Multiple nodes attempt to coordinate DKG during leadership transitions
4. **Malicious Actor**: Attacker with network access runs a rogue coordinator using a cloned configuration

**Required Capabilities:**

- Network access to send messages to signers (standard in any distributed system)
- No cryptographic secrets required if `verify_packet_sigs` is disabled (as in test suite)
- If signature verification is enabled, requires coordinator private key (obtainable through config cloning)

**Attack Complexity:**

Low - attacker simply needs to:
1. Obtain or replicate coordinator configuration
2. Run coordinator instance
3. Call `start_dkg_round()` simultaneously with legitimate coordinator

The test infrastructure demonstrates that multiple coordinators are explicitly supported in the architecture, indicating this is a valid deployment scenario.

**Estimated Probability:**

High in production environments where:
- Multiple nodes run coordinator logic for redundancy
- Network partitions or timing issues cause simultaneous DKG initiation
- Operational errors during deployment or configuration management

## Recommendation

Add duplicate `dkg_id` detection to the signer's `dkg_begin()` function, mirroring the coordinator's implementation:

```rust
fn dkg_begin<R: RngCore + CryptoRng>(
    &mut self,
    dkg_begin: &DkgBegin,
    rng: &mut R,
) -> Result<Vec<Message>, Error> {
    // Add duplicate detection
    if self.dkg_id == dkg_begin.dkg_id && self.state != State::Idle {
        // Already processing this DKG round
        info!(
            dkg_id = %dkg_begin.dkg_id,
            "Received duplicate DkgBegin, ignoring"
        );
        return Ok(vec![]);
    }
    
    self.reset(dkg_begin.dkg_id, rng);
    self.move_to(State::DkgPublicDistribute)?;
    self.dkg_public_begin(rng)
}
```

Additionally, ensure proper coordinator authentication is enabled in production (`verify_packet_sigs = true`) and implement operational safeguards to prevent multiple coordinators with identical configurations from running simultaneously.

## Proof of Concept

The following test demonstrates the vulnerability by allowing two coordinators to send concurrent `DkgBegin` messages:

```rust
#[test]
fn test_concurrent_coordinator_dkg_race() {
    let (mut coordinators, mut signers) = setup::<FireCoordinator, v2::Signer>(3, 1);
    let mut rng = thread_rng();
    
    // Both coordinators initiate DKG independently
    let msg_a = coordinators[0].start_dkg_round(None).unwrap();
    let msg_b = coordinators[1].start_dkg_round(None).unwrap();
    
    // Send both messages to signers (simulating network race)
    let mut shares_a = Vec::new();
    let mut shares_b = Vec::new();
    
    for signer in &mut signers {
        // Signer receives message from coordinator A
        let response_a = signer.process(&msg_a, &mut rng).unwrap();
        shares_a.extend(response_a);
        
        // Signer receives message from coordinator B (resets state!)
        let response_b = signer.process(&msg_b, &mut rng).unwrap();
        shares_b.extend(response_b);
    }
    
    // Coordinators collect shares - due to timing, they get different sets
    for share in &shares_a {
        let _ = coordinators[0].process(&Packet { sig: Signature::default(), msg: share.clone() });
    }
    
    for share in &shares_b {
        let _ = coordinators[1].process(&Packet { sig: Signature::default(), msg: share.clone() });
    }
    
    // Verify: coordinators compute DIFFERENT aggregate public keys
    let key_a = coordinators[0].get_aggregate_public_key();
    let key_b = coordinators[1].get_aggregate_public_key();
    
    assert_ne!(key_a, key_b, "Consensus failure: different aggregate keys");
}
```

This test would fail due to the vulnerability, demonstrating that concurrent coordinators produce consensus failures.

### Citations

**File:** src/state_machine/signer/mod.rs (L417-432)
```rust
    pub fn reset<T: RngCore + CryptoRng>(&mut self, dkg_id: u64, rng: &mut T) {
        self.dkg_id = dkg_id;
        self.commitments.clear();
        self.decrypted_shares.clear();
        self.decryption_keys.clear();
        self.invalid_private_shares.clear();
        self.public_nonces.clear();
        self.signer.reset_polys(rng);
        self.dkg_public_shares.clear();
        self.dkg_private_shares.clear();
        self.dkg_private_begin_msg = None;
        self.dkg_end_begin_msg = None;
        self.kex_private_key = Scalar::random(rng);
        self.kex_public_keys.clear();
        self.state = State::Idle;
    }
```

**File:** src/state_machine/signer/mod.rs (L844-855)
```rust
    fn dkg_begin<R: RngCore + CryptoRng>(
        &mut self,
        dkg_begin: &DkgBegin,
        rng: &mut R,
    ) -> Result<Vec<Message>, Error> {
        self.reset(dkg_begin.dkg_id, rng);
        self.move_to(State::DkgPublicDistribute)?;

        //let _party_state = self.signer.save();

        self.dkg_public_begin(rng)
    }
```

**File:** src/v2.rs (L583-585)
```rust
    fn reset_polys<RNG: RngCore + CryptoRng>(&mut self, rng: &mut RNG) {
        self.f = Some(VSS::random_poly(self.threshold - 1, rng));
    }
```

**File:** src/state_machine/coordinator/frost.rs (L73-82)
```rust
                State::Idle => {
                    // Did we receive a coordinator message?
                    if let Message::DkgBegin(dkg_begin) = &packet.msg {
                        if self.current_dkg_id == dkg_begin.dkg_id {
                            // We have already processed this DKG round
                            return Ok((None, None));
                        }
                        // use dkg_id from DkgBegin
                        let packet = self.start_dkg_round(Some(dkg_begin.dkg_id))?;
                        return Ok((Some(packet), None));
```

**File:** src/state_machine/coordinator/fire.rs (L493-506)
```rust
            let have_shares = self
                .dkg_public_shares
                .contains_key(&dkg_public_shares.signer_id);

            if have_shares {
                info!(signer_id = %dkg_public_shares.signer_id, "received duplicate DkgPublicShares");
                return Ok(());
            }

            self.dkg_wait_signer_ids
                .remove(&dkg_public_shares.signer_id);

            self.dkg_public_shares
                .insert(dkg_public_shares.signer_id, dkg_public_shares.clone());
```

**File:** src/state_machine/coordinator/mod.rs (L725-734)
```rust
        for (i, coordinator) in coordinators.iter_mut().enumerate() {
            for inbound_message in &inbound_messages {
                let (outbound_message, outbound_result) = coordinator.process(inbound_message)?;
                // Only propogate a single coordinator's messages and results
                if i == 0 {
                    messages.extend(outbound_message);
                    results.extend(outbound_result);
                }
            }
        }
```
