### Title
DKG Key Loss Through Repeated compute_secret() Calls with Invalid Shares

### Summary
The `Party::compute_secret()` function immediately sets `private_key` and `group_key` to zero before validating input shares. If a signer successfully completes DKG and then processes a second DKG round (triggered by a replayed or new `DkgBegin` message), calling `compute_secret()` again with invalid shares will zero out the previously valid keys, even when validation fails. This results in permanent key loss and prevents the signer from producing valid signatures.

### Finding Description

**Exact Code Location:**
- `src/v1.rs`, function `Party::compute_secret()`, lines 156-157
- `src/v2.rs`, function `Party::compute_secret()`, lines 129-130 [1](#0-0) 

**Root Cause:**
The `compute_secret()` function unconditionally clears the `private_key` and `group_key` at the beginning of execution, before any input validation occurs. [2](#0-1)  If subsequent validation fails (e.g., bad public shares at line 169 or bad private shares at line 202), the function returns an error, but the keys remain at their zeroed values. There is no mechanism to restore the previous valid keys.

**State Machine Failure to Protect Keys:**
The signer state machine does not check whether DKG has already been successfully completed before processing a new `DkgBegin` message. [3](#0-2)  The `dkg_begin()` function unconditionally calls `reset()` which clears DKG round state but does NOT clear the derived keys from the underlying signer. [4](#0-3) 

When the second DKG round progresses and eventually calls `compute_secrets()`, [5](#0-4)  each party's `compute_secret()` is invoked, zeroing the keys before validation.

**Why Existing Mitigations Fail:**
The coordinator implementation does check for duplicate `dkg_id` values to prevent reprocessing, [6](#0-5)  but the signer implementation has no such check. The signer will process any `DkgBegin` message regardless of whether it has already successfully completed DKG.

The same vulnerability exists in the v2 implementation. [7](#0-6) 

### Impact Explanation

**Specific Harm:**
When a signer loses its keys, it can no longer produce valid signature shares for the threshold signature scheme. If multiple signers are affected, the system may fall below the signing threshold and be unable to produce valid group signatures.

**Quantified Impact:**
- Single signer affected: Reduces available signers for threshold signatures
- Multiple signers affected: If the number of valid signers drops below the threshold `t`, the system cannot produce any valid signatures
- This maps to **Low severity** per the defined scope: "Any remotely-exploitable denial of service in a node"
- Could escalate to network-level DoS if enough signers (>10% of miners) are affected simultaneously

**Affected Parties:**
Any WSTS signer that has successfully completed DKG and subsequently processes a malicious or replayed `DkgBegin` message followed by invalid DKG shares.

### Likelihood Explanation

**Required Attacker Capabilities:**
- Network access to send messages to WSTS signers via the protocol's message passing interface
- Ability to construct and send protocol messages (`DkgBegin`, `DkgPublicShares`, `DkgPrivateShares`, `DkgPrivateBegin`, `DkgEndBegin`)
- No cryptographic breaks required
- No special privileges beyond network access to the WSTS protocol

**Attack Complexity:**
Medium. The attacker must:
1. Send a `DkgBegin` message with any `dkg_id` (new or replayed)
2. Progress through the DKG state machine by sending the required sequence of messages
3. Provide invalid shares that will fail validation in `compute_secret()`
4. Trigger `dkg_ended()` which calls `compute_secrets()`

**Economic Feasibility:**
Low cost - only requires network bandwidth to send protocol messages.

**Detection Risk:**
Medium - the attack produces `DkgEnd` messages with failure status, which would be logged, but the permanent key loss may not be immediately obvious.

**Estimated Probability:**
High if attacker has network access, as the signer has no protection against processing multiple DKG rounds.

### Recommendation

**Primary Fix:**
Preserve existing keys before attempting to compute new ones. Only overwrite keys after successful validation:

```rust
pub fn compute_secret(
    &mut self,
    private_shares: HashMap<u32, Scalar>,
    public_shares: &HashMap<u32, PolyCommitment>,
    ctx: &[u8],
) -> Result<(), DkgError> {
    // Save existing keys
    let old_private_key = self.private_key;
    let old_group_key = self.group_key;
    
    // Temporarily clear for computation
    let mut new_private_key = Scalar::zero();
    let mut new_group_key = Point::zero();

    let threshold: usize = self.threshold.try_into()?;
    let mut bad_ids = Vec::new();
    for (i, comm) in public_shares.iter() {
        if !check_public_shares(comm, threshold, ctx) {
            bad_ids.push(*i);
        } else {
            new_group_key += comm.poly[0];
        }
    }
    if !bad_ids.is_empty() {
        // Validation failed - keys remain unchanged
        return Err(DkgError::BadPublicShares(bad_ids));
    }

    // ... continue validation with new_private_key ...
    
    // Only update keys after all validation passes
    new_private_key = private_shares.values().sum();
    self.private_key = new_private_key;
    self.public_key = self.private_key * G;
    self.group_key = new_group_key;

    Ok(())
}
```

**Additional Mitigation:**
Add a guard in the signer state machine to prevent processing duplicate or new DKG rounds after successful completion:

```rust
fn dkg_begin<R: RngCore + CryptoRng>(
    &mut self,
    dkg_begin: &DkgBegin,
    rng: &mut R,
) -> Result<Vec<Message>, Error> {
    // Check if we already processed this dkg_id
    if self.dkg_id == dkg_begin.dkg_id && self.state == State::Idle {
        return Ok(vec![]); // Already processed
    }
    
    self.reset(dkg_begin.dkg_id, rng);
    self.move_to(State::DkgPublicDistribute)?;
    self.dkg_public_begin(rng)
}
```

**Testing Recommendations:**
1. Add test that calls `compute_secret()` twice with valid shares first, then invalid shares
2. Verify keys remain unchanged after failed second call
3. Test state machine rejects duplicate `dkg_id` values
4. Test that signers can still sign after failed DKG attempts

**Deployment Considerations:**
This fix must be deployed to all signers before they complete their first DKG to prevent key loss. Apply the same fix to both v1 and v2 implementations.

### Proof of Concept

**Exploitation Algorithm:**

1. **Setup:** Assume signer has completed DKG successfully with valid keys
   - `signer.parties[0].private_key = valid_key`
   - `signer.parties[0].group_key = valid_group_key`

2. **Trigger Second DKG Round:**
   ```
   Send: DkgBegin { dkg_id: signer.dkg_id + 1 }
   Result: Signer calls reset() and moves to DkgPublicDistribute state
   ```

3. **Send Invalid DKG Shares:**
   ```
   Send: DkgPublicShares with malformed polynomial commitments
   Send: DkgPrivateBegin { signer_ids: [...] }
   Send: DkgPrivateShares with shares that don't match commitments
   Send: DkgEndBegin { signer_ids: [...] }
   ```

4. **Trigger Key Computation:**
   ```
   Signer's can_dkg_end() returns true
   Signer calls dkg_ended() which calls signer.compute_secrets()
   This calls party.compute_secret() for each party
   ```

5. **Result:**
   ```
   Line 156-157: private_key and group_key are set to zero
   Line 169 or 202: Validation fails, returns error
   Final state: signer.parties[0].private_key = Scalar::zero()
                 signer.parties[0].group_key = Point::zero()
   ```

**Expected vs Actual Behavior:**
- **Expected:** If `compute_secret()` fails validation, existing keys should remain unchanged
- **Actual:** Keys are zeroed at the start of the function and remain zero after validation failure

**Reproduction:**
Use the test framework to create a signer, complete successful DKG, then send a second DKG round with invalid shares. Verify that after the failed second round, the signer's keys are zero and it cannot produce valid signature shares.

### Citations

**File:** src/v1.rs (L156-209)
```rust
        self.private_key = Scalar::zero();
        self.group_key = Point::zero();

        let threshold: usize = self.threshold.try_into()?;
        let mut bad_ids = Vec::new(); //: Vec<u32> = polys
        for (i, comm) in public_shares.iter() {
            if !check_public_shares(comm, threshold, ctx) {
                bad_ids.push(*i);
            } else {
                self.group_key += comm.poly[0];
            }
        }
        if !bad_ids.is_empty() {
            return Err(DkgError::BadPublicShares(bad_ids));
        }

        let mut missing_shares = Vec::new();
        for i in public_shares.keys() {
            if private_shares.get(i).is_none() {
                missing_shares.push((self.id, *i));
            }
        }
        if !missing_shares.is_empty() {
            return Err(DkgError::MissingPrivateShares(missing_shares));
        }

        // batch verification requires that we multiply each term by a random scalar in order to
        // prevent a bypass attack.  Doing this using p256k1's MultiMult trait is problematic,
        // because it needs to have every term available so it can return references to them,
        // so we wouldn't be able to save any memory since we'd have to multiple each polynomial
        // coefficient by a different random scalar.
        // we could implement a MultiMultCopy trait that allows us to do the multiplication inline,
        // at the cost of many copies, or use large amounts of memory and do a standard multimult.
        // Or we could just verify each set of public and private shares separately, using extra CPU
        let mut bad_shares = Vec::new();
        for (i, s) in private_shares.iter() {
            if let Some(comm) = public_shares.get(i) {
                if s * G != compute::poly(&self.id(), &comm.poly)? {
                    bad_shares.push(*i);
                }
            } else {
                warn!("unable to check private share from {}: no corresponding public share, even though we checked for it above", i);
            }
        }

        if !bad_shares.is_empty() {
            return Err(DkgError::BadPrivateShares(bad_shares));
        }

        self.private_key = private_shares.values().sum();
        self.public_key = self.private_key * G;

        Ok(())
    }
```

**File:** src/state_machine/signer/mod.rs (L417-432)
```rust
    pub fn reset<T: RngCore + CryptoRng>(&mut self, dkg_id: u64, rng: &mut T) {
        self.dkg_id = dkg_id;
        self.commitments.clear();
        self.decrypted_shares.clear();
        self.decryption_keys.clear();
        self.invalid_private_shares.clear();
        self.public_nonces.clear();
        self.signer.reset_polys(rng);
        self.dkg_public_shares.clear();
        self.dkg_private_shares.clear();
        self.dkg_private_begin_msg = None;
        self.dkg_end_begin_msg = None;
        self.kex_private_key = Scalar::random(rng);
        self.kex_public_keys.clear();
        self.state = State::Idle;
    }
```

**File:** src/state_machine/signer/mod.rs (L612-616)
```rust
            match self.signer.compute_secrets(
                &self.decrypted_shares,
                &self.commitments,
                &self.dkg_id.to_be_bytes(),
            ) {
```

**File:** src/state_machine/signer/mod.rs (L844-854)
```rust
    fn dkg_begin<R: RngCore + CryptoRng>(
        &mut self,
        dkg_begin: &DkgBegin,
        rng: &mut R,
    ) -> Result<Vec<Message>, Error> {
        self.reset(dkg_begin.dkg_id, rng);
        self.move_to(State::DkgPublicDistribute)?;

        //let _party_state = self.signer.save();

        self.dkg_public_begin(rng)
```

**File:** src/state_machine/coordinator/fire.rs (L231-233)
```rust
                        if self.current_dkg_id == dkg_begin.dkg_id {
                            // We have already processed this DKG round
                            return Ok((None, None));
```

**File:** src/v2.rs (L129-130)
```rust
        self.private_keys.clear();
        self.group_key = Point::zero();
```
