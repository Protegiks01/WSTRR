### Title
Coordinator Assumes Message Ordering Without Validation in v1 Aggregation

### Summary
The coordinator implementations (frost and fire) assume that nonces and signature shares within a signer's messages are ordered consistently, but do not validate this assumption. A malicious v1 signer can exploit this by sending NonceResponse and SignatureShareResponse messages with mismatched internal ordering, causing signature verification failures and preventing consensus.

### Finding Description
**Location:** 
- [1](#0-0) 
- [2](#0-1) 

**Root Cause:**
The coordinator collects nonces and signature shares by flat-mapping responses in signer ID order. When aggregating, it passes these arrays to the v1 aggregator, which assumes `nonces[i]` corresponds to `sig_shares[i]`. However, the coordinator never validates that the internal ordering within each signer's NonceResponse matches the ordering in their SignatureShareResponse. [3](#0-2) 

The coordinator only validates that the SET of key_ids matches the expected set, not the ORDER: [4](#0-3) 

**Why Existing Mitigations Fail:**
The coordinator checks `*signer_key_ids != sig_share_response_key_ids` using HashSet comparison, which validates set equality but ignores ordering. No code validates that nonces and shares are in matching positional order.

### Impact Explanation
**Specific Harm:**
A malicious v1 signer can send crafted messages with reordered signature shares relative to their nonces. When the aggregator processes these mismatched arrays, it pairs wrong nonces with wrong shares during verification via `compute::intermediate`: [5](#0-4) 

The verification logic assumes `nonces[i]` corresponds to `party_ids[i]`: [6](#0-5) 

**Impact:**
- **Denial of Service:** All signature attempts fail when the malicious signer participates, preventing block production or transaction signing
- **Transient Consensus Failures:** The system cannot create valid signatures until the malicious signer is identified and removed
- **Affects:** All v1 deployments where signers control multiple key IDs (parties)

**Severity:** Medium - Maps to "transient consensus failures" as signatures cannot be completed.

### Likelihood Explanation
**Required Capabilities:**
- Attacker must be a legitimate signer with valid DKG shares
- Attacker can construct custom Packet messages with reordered fields
- No cryptographic breaks required

**Attack Complexity:** Low
1. Participate in DKG normally to become legitimate signer
2. During signing round, send NonceResponse with key_ids=[1,2,3] and nonces=[N1,N2,N3]
3. Send SignatureShareResponse with reordered shares where signature_shares[0].id=3, signature_shares[1].id=2, signature_shares[2].id=1

**Detection:** Easily detectable as all signatures fail when attacker participates, but system lacks mechanism to distinguish this from other signature failures.

**Probability:** High if attacker is motivated, as it requires only message crafting capability.

### Recommendation
**Primary Fix:** Add ordering validation in coordinator's `gather_sig_shares` function:

```rust
// After collecting signature shares, validate ordering matches nonces
let nonce_key_ids = &nonce_response.key_ids;
let share_key_ids: Vec<u32> = sig_share_response.signature_shares
    .iter()
    .map(|share| share.id)
    .collect();
    
if nonce_key_ids != &share_key_ids {
    warn!(signer_id = %sig_share_response.signer_id, 
          "SignatureShareResponse ordering doesn't match NonceResponse");
    return Err(Error::MaliciousSigner(sig_share_response.signer_id));
}
```

**Alternative Mitigation:** Have aggregator use SignatureShare.id field to match shares with correct nonces instead of assuming positional correspondence.

**Testing:** Add test cases with intentionally reordered signature shares to verify detection.

### Proof of Concept
**Attack Steps:**
1. Deploy v1 signer with key_ids [1, 2, 3]
2. Participate in DKG normally
3. In signing round, receive NonceRequest from coordinator
4. Generate honest nonces N1, N2, N3 for parties 1, 2, 3
5. Construct NonceResponse: `{signer_id: 0, key_ids: [1,2,3], nonces: [N1,N2,N3]}`
6. Receive SignatureShareRequest
7. Generate honest shares S1, S2, S3 for parties 1, 2, 3
8. Construct malicious SignatureShareResponse: `{signer_id: 0, signature_shares: [S3, S2, S1]}`
   where S3.id=3, S2.id=2, S1.id=1 (reordered by id)
9. Coordinator aggregates: nonces=[N1,N2,N3], shares=[S3,S2,S1]
10. Aggregator extracts signers=[3,2,1], pairs with nonces=[N1,N2,N3]
11. Computes Rs assuming N1→party3, N2→party2, N3→party1 (incorrect)
12. Signature verification fails
13. Signature cannot be created, preventing consensus

**Expected Behavior:** Signature completes successfully  
**Actual Behavior:** Signature fails, DoS achieved

### Citations

**File:** src/state_machine/coordinator/frost.rs (L631-641)
```rust
            let mut sig_share_response_key_ids = HashSet::new();
            for sig_share in &sig_share_response.signature_shares {
                for key_id in &sig_share.key_ids {
                    sig_share_response_key_ids.insert(*key_id);
                }
            }

            if *signer_key_ids != sig_share_response_key_ids {
                warn!(signer_id = %sig_share_response.signer_id, "SignatureShareResponse key_ids didn't match config");
                return Ok(());
            }
```

**File:** src/state_machine/coordinator/frost.rs (L664-684)
```rust
        if self.ids_to_await.is_empty() {
            // Calculate the aggregate signature
            let nonce_responses = (0..self.config.num_signers)
                .map(|i| self.public_nonces[&i].clone())
                .collect::<Vec<NonceResponse>>();

            let nonces = nonce_responses
                .iter()
                .flat_map(|nr| nr.nonces.clone())
                .collect::<Vec<PublicNonce>>();

            let key_ids = nonce_responses
                .iter()
                .flat_map(|nr| nr.key_ids.clone())
                .collect::<Vec<u32>>();

            let shares = &self
                .public_nonces
                .iter()
                .flat_map(|(i, _)| self.signature_shares[i].clone())
                .collect::<Vec<SignatureShare>>();
```

**File:** src/state_machine/coordinator/fire.rs (L1115-1135)
```rust
            let nonce_responses = message_nonce
                .public_nonces
                .values()
                .cloned()
                .collect::<Vec<NonceResponse>>();

            let nonces = nonce_responses
                .iter()
                .flat_map(|nr| nr.nonces.clone())
                .collect::<Vec<PublicNonce>>();

            let key_ids = nonce_responses
                .iter()
                .flat_map(|nr| nr.key_ids.clone())
                .collect::<Vec<u32>>();

            let shares = message_nonce
                .public_nonces
                .iter()
                .flat_map(|(i, _)| self.signature_shares[i].clone())
                .collect::<Vec<SignatureShare>>();
```

**File:** src/compute.rs (L85-96)
```rust
pub fn intermediate(msg: &[u8], party_ids: &[u32], nonces: &[PublicNonce]) -> (Vec<Point>, Point) {
    let rhos: Vec<Scalar> = party_ids
        .iter()
        .map(|&i| binding(&id(i), nonces, msg))
        .collect();
    let R_vec: Vec<Point> = zip(nonces, rhos)
        .map(|(nonce, rho)| nonce.D + rho * nonce.E)
        .collect();

    let R = R_vec.iter().fold(Point::zero(), |R, &R_i| R + R_i);
    (R_vec, R)
}
```

**File:** src/v1.rs (L399-416)
```rust
        for i in 0..sig_shares.len() {
            let id = compute::id(sig_shares[i].id);
            let public_key = match compute::poly(&id, &self.poly) {
                Ok(p) => p,
                Err(_) => {
                    bad_party_keys.push(sig_shares[i].id);
                    Point::zero()
                }
            };

            let z_i = sig_shares[i].z_i;

            if z_i * G
                != r_sign * Rs[i]
                    + cx_sign * (compute::lambda(sig_shares[i].id, &signers) * c * public_key)
            {
                bad_party_sigs.push(sig_shares[i].id);
            }
```
