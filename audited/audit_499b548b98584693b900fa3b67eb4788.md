### Title
Coordinator Panic Due to Premature Wait List Removal in Signature Share Gathering

### Summary
The FIRE coordinator removes signers from the wait list before validating their signature share responses, creating an accounting mismatch between signers who sent nonces and signers with valid signature shares. When aggregation proceeds after validation failures, the coordinator attempts to access signature shares for signers who were removed but never had their shares stored, causing a panic that crashes the coordinator.

### Finding Description

**Exact Code Location:** [1](#0-0) 

The coordinator removes the signer from `sign_wait_signer_ids` immediately after confirming they are in the wait list, but BEFORE performing critical validation checks. [2](#0-1) 

These validation checks occur after wait list removal and can fail with errors, but the signer has already been removed from the wait list. When validation fails, signature shares are not inserted. [3](#0-2) 

During aggregation, the code iterates over all signers in `public_nonces` and accesses `signature_shares[i]` using bracket notation on a BTreeMap. This panics if the key doesn't exist.

**Root Cause:**
The coordinator uses two separate data structures to track signing progress:
1. `public_nonces` - contains all signers who sent nonces during nonce gathering
2. `signature_shares` - contains only signers whose signature share validation succeeded

The code incorrectly assumes these sets are identical when building the aggregation input. The premature removal from wait list (line 1042-1044) before validation (lines 1046-1076) breaks this invariant.

**Why Existing Mitigations Fail:** [4](#0-3) 

When validation fails, an error is returned to the caller. However, the coordinator state remains in `SigShareGather` with corrupted accounting (signer removed from wait list but not in signature_shares). If the application continues processing subsequent packets (a reasonable behavior when errors may be recoverable), the wait list eventually becomes empty and aggregation proceeds with the corrupted state.

### Impact Explanation

**Specific Harm:**
A malicious signer can trigger a panic in the coordinator by sending a signature share response that fails validation (e.g., mismatched key_ids, missing public key). This causes the Rust program to unwind and terminate, crashing the coordinator node.

**Quantified Impact:**
- Single malicious signer can crash any coordinator
- Attack works against any threshold configuration
- No recovery without coordinator restart
- Affects all dependent systems waiting for threshold signatures

**Who is Affected:**
Any application using the WSTS FIRE coordinator for threshold signature generation. In the Stacks blockchain context, this impacts signers coordinating multi-sig transactions or DKG operations.

**Severity Justification:**
This maps to **Low severity** under the protocol scope definitions: "Any remotely-exploitable denial of service in a node." While a coordinator crash is disruptive, it does not directly cause fund loss, invalid signature acceptance, or chain-level consensus failures. The coordinator can be restarted and the signing round retried.

### Likelihood Explanation

**Required Attacker Capabilities:**
- Must be a registered signer in the WSTS configuration
- Must participate in nonce gathering phase (send valid nonces)
- Must be able to send malformed signature share responses

**Attack Complexity:**
Very low. The attacker simply needs to:
1. Participate normally in nonce gathering
2. Send a signature share response with invalid key_ids, wrong public key reference, or other validation-triggering malformation
3. Wait for other honest signers to complete

**Economic Feasibility:**
Trivial. No computational cost beyond normal signing participation. A single malformed packet triggers the vulnerability.

**Detection Risk:**
Low. The attack appears as a normal validation failure followed by a crash. Without detailed logging, it's indistinguishable from a software bug.

**Probability of Success:**
~100%. The vulnerability is deterministic and requires no race conditions or timing dependencies.

### Recommendation

**Primary Fix:**
Move the wait list removal to occur AFTER all validation checks pass and signature shares are successfully inserted: [5](#0-4) 

The removal at lines 1042-1044 should be moved to after line 1097, following the pattern used in the FROST coordinator: [6](#0-5) 

**Alternative Mitigation:**
If immediate wait list removal is required for other reasons, change the aggregation logic to use `.get()` instead of bracket indexing and skip signers with missing signature shares, or verify that all signers in public_nonces have corresponding entries in signature_shares before proceeding.

**Testing Recommendations:**
1. Add unit test where one signer sends invalid signature shares (wrong key_ids)
2. Verify coordinator does not panic
3. Verify signer remains in wait list after validation failure
4. Test timeout behavior with validation failures

**Deployment Considerations:**
This is a critical fix that should be deployed immediately. Existing deployments are vulnerable to trivial DoS attacks from any participating signer.

### Proof of Concept

**Exploitation Steps:**

1. **Setup:** Coordinator configured with threshold=2, three signers (IDs 1, 2, 3), each with valid key_ids
2. **Nonce Phase:** All three signers send valid nonces, added to public_nonces and sign_wait_signer_ids
3. **Attack:** Signer 2 (attacker) sends SignatureShareResponse with key_ids that don't match config
   - Line 1015-1025: Signer 2 is in wait list ✓
   - Line 1042-1044: Signer 2 removed from sign_wait_signer_ids
   - Line 1073-1075: Validation fails on key_id mismatch
   - Error returned, signature_shares[2] is NOT created
4. **Continuation:** Signer 1 sends valid shares → signature_shares[1] created
5. **Continuation:** Signer 3 sends valid shares → signature_shares[3] created  
6. **Trigger:** sign_wait_signer_ids is now empty
7. **Crash:** Line 1113 check passes, aggregation begins
   - Line 1131-1135 iterates: i ∈ {1, 2, 3} from public_nonces
   - Accesses signature_shares[1] ✓
   - Accesses signature_shares[2] → **PANIC: key not found**

**Expected Behavior:**
Coordinator should either:
- Keep signer 2 in wait list and timeout waiting for valid response, OR
- Skip signer 2 during aggregation and proceed with signers 1 and 3 only

**Actual Behavior:**
Coordinator panics with index out of bounds on BTreeMap, terminating the process.

**Reproduction:**
Create a test with three signers, have one send signature shares with modified key_ids after successful nonce phase, then have the other two send valid shares. The coordinator will panic during aggregation.

### Citations

**File:** src/state_machine/coordinator/fire.rs (L328-333)
```rust
                    if let Err(e) = self.gather_sig_shares(packet, signature_type) {
                        return Ok((
                            None,
                            Some(OperationResult::SignError(SignError::Coordinator(e))),
                        ));
                    }
```

**File:** src/state_machine/coordinator/fire.rs (L1042-1044)
```rust
        response_info
            .sign_wait_signer_ids
            .remove(&sig_share_response.signer_id);
```

**File:** src/state_machine/coordinator/fire.rs (L1046-1076)
```rust
        // check that the signer_id exists in the config
        let signer_public_keys = &self.config.public_keys.signers;
        if !signer_public_keys.contains_key(&sig_share_response.signer_id) {
            warn!(signer_id = %sig_share_response.signer_id, "No public key in config");
            return Err(Error::MissingPublicKeyForSigner(
                sig_share_response.signer_id,
            ));
        };

        // check that the key_ids match the config
        let Some(signer_key_ids) = self
            .config
            .public_keys
            .signer_key_ids
            .get(&sig_share_response.signer_id)
        else {
            warn!(signer_id = %sig_share_response.signer_id, "No keys IDs configured");
            return Err(Error::MissingKeyIDsForSigner(sig_share_response.signer_id));
        };

        let mut sig_share_response_key_ids = HashSet::new();
        for sig_share in &sig_share_response.signature_shares {
            for key_id in &sig_share.key_ids {
                sig_share_response_key_ids.insert(*key_id);
            }
        }

        if *signer_key_ids != sig_share_response_key_ids {
            warn!(signer_id = %sig_share_response.signer_id, "SignatureShareResponse key_ids didn't match config");
            return Err(Error::BadKeyIDsForSigner(sig_share_response.signer_id));
        }
```

**File:** src/state_machine/coordinator/fire.rs (L1088-1097)
```rust
        self.signature_shares.insert(
            sig_share_response.signer_id,
            sig_share_response.signature_shares.clone(),
        );

        for sig_share in &sig_share_response.signature_shares {
            for key_id in &sig_share.key_ids {
                response_info.sign_recv_key_ids.insert(*key_id);
            }
        }
```

**File:** src/state_machine/coordinator/fire.rs (L1131-1135)
```rust
            let shares = message_nonce
                .public_nonces
                .iter()
                .flat_map(|(i, _)| self.signature_shares[i].clone())
                .collect::<Vec<SignatureShare>>();
```

**File:** src/state_machine/coordinator/frost.rs (L652-656)
```rust
            self.signature_shares.insert(
                sig_share_response.signer_id,
                sig_share_response.signature_shares.clone(),
            );
            self.ids_to_await.remove(&sig_share_response.signer_id);
```
