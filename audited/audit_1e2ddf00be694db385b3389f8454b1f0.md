### Title
Coordinator State Corruption Leading to Denial of Service via Invalid Signature Share Handling

### Summary
The FIRE coordinator removes signers from the wait list before validating their signature shares, creating a state corruption vulnerability. When validation fails, the signer remains marked as "processed" but their shares are absent from the aggregation map. This causes the coordinator to panic when attempting aggregation after all signers have responded, leading to a remotely-exploitable denial of service.

### Finding Description

The vulnerability exists in the FIRE coordinator's `gather_sig_shares` method. The critical flaw is in the ordering of operations: [1](#0-0) 

The signer is removed from `sign_wait_signer_ids` immediately after confirming they are in the wait list, but BEFORE any validation of their signature share response. [2](#0-1) 

Multiple validation checks occur after the wait list modification, including checks for public key existence, key ID configuration, and key ID matching. Any of these can return errors without inserting the shares into `self.signature_shares`. [3](#0-2) 

Shares are only inserted if all validation passes. This creates an inconsistent state where a signer is marked as "processed" (removed from wait list) but their shares are missing from the aggregation map. [4](#0-3) 

When all signers have been processed (wait list is empty), aggregation is triggered. The code collects shares by indexing into `self.signature_shares` using the signer IDs from `public_nonces`. This indexing operation will panic if a signer sent nonces but their shares are missing due to validation failure.

The error handling in the main message processing loop catches validation errors and returns them as `SignError` OperationResults, but the corrupted state persists: [5](#0-4) 

**Root Cause**: The wait list modification is performed optimistically before validation completes, violating the atomic state transition requirement. When validation fails, the state is partially updated (signer removed from wait list) without completing the full operation (inserting shares).

### Impact Explanation

**What breaks**: A malicious signer can crash the coordinator node by sending signature shares with invalid key_ids or other validation failures.

**Specific harm**: 
- The coordinator panics during aggregation, terminating the process
- All honest signers' work (nonce generation and signature share computation) is wasted
- The signing round must be completely restarted
- Repeated attacks can prevent signature generation indefinitely

**Who is affected**: Any WSTS deployment using the FIRE coordinator for multi-party signatures. This includes Stacks blockchain nodes using WSTS for Stacks 2.1+ signer operations.

**Severity justification**: According to the audit scope, this maps to **Low severity** - "Any remotely-exploitable denial of service in a node". A malicious signer (who must be a valid participant with configured public keys) can crash the coordinator, but this does not directly cause chain splits, fund loss, or network-wide outages. However, if this affects critical signing operations (e.g., Stacks block signing), repeated exploitation could impact more than 10% of miners, potentially elevating severity.

### Likelihood Explanation

**Required attacker capabilities**:
- Must be a valid signer with a registered signer_id in the coordinator's configuration
- Must have participated in DKG to send nonces
- No cryptographic breaks required

**Attack complexity**: Low
1. Participate normally in DKG and receive signing requests
2. Send valid NonceResponse when requested
3. When SignatureShareRequest arrives, send SignatureShareResponse with intentionally mismatched key_ids

**Economic feasibility**: Free to execute once positioned as a signer

**Detection risk**: High visibility - the coordinator crash would be immediately apparent, but the malicious signer may not be immediately identifiable if multiple signers are present

**Estimated probability**: High - any authorized signer can trivially execute this attack

### Recommendation

**Primary fix**: Move the wait list removal to occur AFTER all validation checks pass:

```rust
// Perform all validation checks FIRST
if sig_share_response.dkg_id != self.current_dkg_id {
    return Err(Error::BadDkgId(...));
}
if sig_share_response.sign_id != self.current_sign_id {
    return Err(Error::BadSignId(...));
}

// Validate signer_id and key_ids
let signer_public_keys = &self.config.public_keys.signers;
if !signer_public_keys.contains_key(&sig_share_response.signer_id) {
    return Err(Error::MissingPublicKeyForSigner(...));
}
// ... complete all validation ...

if *signer_key_ids != sig_share_response_key_ids {
    return Err(Error::BadKeyIDsForSigner(...));
}

// Check for duplicates
if self.signature_shares.contains_key(&sig_share_response.signer_id) {
    return Ok(()); // Already processed
}

// ONLY AFTER all validation passes, update state atomically:
response_info.sign_wait_signer_ids.remove(&sig_share_response.signer_id);
self.signature_shares.insert(
    sig_share_response.signer_id,
    sig_share_response.signature_shares.clone(),
);
```

**Alternative mitigation**: Use safe indexing in aggregation:
```rust
let shares = message_nonce
    .public_nonces
    .iter()
    .filter_map(|(i, _)| self.signature_shares.get(i))
    .flat_map(|s| s.clone())
    .collect::<Vec<SignatureShare>>();
```

However, this only prevents the panic; the better fix is preventing the inconsistent state.

**Testing recommendations**:
- Add integration test where one signer sends invalid key_ids during signature share phase
- Verify coordinator returns error but remains operational for retry
- Test with FROST coordinator to ensure consistent behavior

**Deployment considerations**: This fix changes state machine behavior and should be deployed with coordinator version bump. Existing deployments should upgrade before malicious actors discover this vulnerability.

### Proof of Concept

**Exploitation steps**:

1. Setup: Deploy FIRE coordinator with threshold=3, 5 signers, 2 keys per signer (10 total keys)

2. Normal flow: Complete DKG successfully, all signers have valid keys

3. Attack during signing:
   - Coordinator sends NonceRequest for message M
   - All 5 signers respond with valid NonceResponse
   - Coordinator sends SignatureShareRequest with nonce_responses from all 5 signers
   - Signers 0, 2, 3, 4 respond with valid SignatureShareResponse
   - Attacker (signer 1) responds with SignatureShareResponse where:
     * `signer_id = 1` (valid)
     * `signature_shares` contain `key_ids = [99, 100]` (invalid - not in config)

4. Expected behavior: Coordinator should detect malicious signer and exclude them, continuing with remaining signers if threshold met

5. Actual behavior:
   - Coordinator processes signer 1's response
   - Removes signer 1 from wait list (line 1042-1044)
   - Validation fails at line 1073-1076 (key_ids mismatch)
   - Returns `OperationResult::SignError(SignError::Coordinator(Error::BadKeyIDsForSigner(1)))`
   - State is corrupted: signer 1 marked as processed but shares missing
   - Coordinator processes remaining signers 2, 3, 4 successfully
   - Wait list becomes empty
   - Aggregation triggered at line 1113
   - Line 1134 executes: `self.signature_shares[1].clone()`
   - **PANIC**: "no entry found for key" - coordinator crashes

**Parameter values**:
- num_signers: 5
- keys_per_signer: 2
- threshold: 6 (out of 10 total keys)
- Malicious signer: 1
- Invalid key_ids sent: any values not in signer 1's configured key_ids

**Reproduction**: Use the existing test infrastructure in `src/state_machine/coordinator/mod.rs` and modify one signer to send invalid key_ids during `SignatureShareResponse`.

### Citations

**File:** src/state_machine/coordinator/fire.rs (L327-336)
```rust
                State::SigShareGather(signature_type) => {
                    if let Err(e) = self.gather_sig_shares(packet, signature_type) {
                        return Ok((
                            None,
                            Some(OperationResult::SignError(SignError::Coordinator(e))),
                        ));
                    }
                    if self.state == State::SigShareGather(signature_type) {
                        // We need more data
                        return Ok((None, None));
```

**File:** src/state_machine/coordinator/fire.rs (L1042-1044)
```rust
        response_info
            .sign_wait_signer_ids
            .remove(&sig_share_response.signer_id);
```

**File:** src/state_machine/coordinator/fire.rs (L1046-1076)
```rust
        // check that the signer_id exists in the config
        let signer_public_keys = &self.config.public_keys.signers;
        if !signer_public_keys.contains_key(&sig_share_response.signer_id) {
            warn!(signer_id = %sig_share_response.signer_id, "No public key in config");
            return Err(Error::MissingPublicKeyForSigner(
                sig_share_response.signer_id,
            ));
        };

        // check that the key_ids match the config
        let Some(signer_key_ids) = self
            .config
            .public_keys
            .signer_key_ids
            .get(&sig_share_response.signer_id)
        else {
            warn!(signer_id = %sig_share_response.signer_id, "No keys IDs configured");
            return Err(Error::MissingKeyIDsForSigner(sig_share_response.signer_id));
        };

        let mut sig_share_response_key_ids = HashSet::new();
        for sig_share in &sig_share_response.signature_shares {
            for key_id in &sig_share.key_ids {
                sig_share_response_key_ids.insert(*key_id);
            }
        }

        if *signer_key_ids != sig_share_response_key_ids {
            warn!(signer_id = %sig_share_response.signer_id, "SignatureShareResponse key_ids didn't match config");
            return Err(Error::BadKeyIDsForSigner(sig_share_response.signer_id));
        }
```

**File:** src/state_machine/coordinator/fire.rs (L1088-1091)
```rust
        self.signature_shares.insert(
            sig_share_response.signer_id,
            sig_share_response.signature_shares.clone(),
        );
```

**File:** src/state_machine/coordinator/fire.rs (L1113-1135)
```rust
        if message_nonce.sign_wait_signer_ids.is_empty() {
            // Calculate the aggregate signature
            let nonce_responses = message_nonce
                .public_nonces
                .values()
                .cloned()
                .collect::<Vec<NonceResponse>>();

            let nonces = nonce_responses
                .iter()
                .flat_map(|nr| nr.nonces.clone())
                .collect::<Vec<PublicNonce>>();

            let key_ids = nonce_responses
                .iter()
                .flat_map(|nr| nr.key_ids.clone())
                .collect::<Vec<u32>>();

            let shares = message_nonce
                .public_nonces
                .iter()
                .flat_map(|(i, _)| self.signature_shares[i].clone())
                .collect::<Vec<SignatureShare>>();
```
