### Title
Coordinator Consensus Failure Due to DkgPrivateGather Timeout Race Condition

### Summary
When the DkgPrivateGather timeout fires with dkg_threshold met, the coordinator continues the protocol with only the signers who have responded at that moment. Different coordinators may have received different sets of DkgPrivateShares due to network timing variations, causing each to proceed with a different subset of signers. This results in different coordinators computing different aggregate public keys, creating a consensus failure that splits the network.

### Finding Description

The vulnerability exists in the FIRE coordinator's timeout handling logic. [1](#0-0) 

When the DkgPrivateGather timeout fires, the coordinator checks if enough keys have been received by calling `compute_dkg_private_size()`. [2](#0-1)  This function counts the total number of key IDs from signers who have submitted DkgPrivateShares. [3](#0-2) 

If `dkg_size >= dkg_threshold`, the coordinator continues by calling `private_shares_gathered()` and `start_dkg_end()`, which transitions to DkgEndDistribute and then DkgEndGather states. Critically, `start_dkg_end()` sets `dkg_wait_signer_ids` to only include signers in `dkg_private_shares.keys()` - those who responded before the timeout. [4](#0-3) 

The aggregate public key is later computed from the polynomial constants of only these signers who completed the DkgEnd phase. [5](#0-4) 

**Root Cause**: The coordinator makes an independent, time-sensitive decision about which signers to include in the final DKG based on local message arrival times, without coordination with other coordinators. Different coordinators will have different views of which DkgPrivateShares have arrived at the moment their timeout fires.

**Why Existing Mitigations Fail**: The `dkg_threshold` parameter is designed to allow partial participation, but it assumes all coordinators will agree on which signers participate. There is no mechanism to ensure coordinators synchronize on the signer set when using timeouts.

### Impact Explanation

This vulnerability directly violates the critical DKG invariant: "Group public key must equal the sum of valid polynomial constants." Different coordinators will sum different sets of polynomial constants, producing different group public keys.

**Specific Harm**:
- Different coordinators compute different aggregate public keys for the same DKG round
- Subsequent signing operations will fail because nodes cannot agree on which public key to use
- Network splits into partitions, each believing they have the correct group key
- All signatures produced become invalid from the perspective of nodes using a different key

**Quantification**: Consider a 10-signer network with `dkg_threshold=28` keys (7 keys per signer, requiring 4 signers):
- Coordinator A's timeout fires when it has received shares from signers {0,1,2,3}
- Coordinator B's timeout fires 100ms later when it has received shares from signers {0,1,2,4}
- A computes: `agg_key_A = poly[0]_0 + poly[0]_1 + poly[0]_2 + poly[0]_3`
- B computes: `agg_key_B = poly[0]_0 + poly[0]_1 + poly[0]_2 + poly[0]_4`
- `agg_key_A ≠ agg_key_B` with overwhelming probability

**Affected Parties**: All nodes in the network using WSTS with FIRE coordinators and configured DKG timeouts.

**Severity Justification**: This qualifies as **HIGH severity** under the protocol scope definition: "Any unintended chain split or network partition." Different nodes cannot agree on the group public key, making it impossible to validate signatures consistently across the network.

### Likelihood Explanation

**Required Attacker Capabilities**: 
- No attacker needed - this is a race condition that occurs naturally with network latency
- Alternatively, an attacker with network-level access could selectively delay messages to increase likelihood

**Attack Complexity**: LOW
- Naturally occurs whenever coordinators have different timeout values or experience different network conditions
- No cryptographic attacks required
- No privileged access required

**Economic Feasibility**: FREE (natural occurrence) or LOW COST (if deliberately triggered)
- Natural: Simply configure aggressive timeouts or operate in high-latency environments
- Deliberate: Basic network delay attacks (100-1000ms delays sufficient)

**Detection Risk**: HIGH - difficult to detect as it appears as normal timeout behavior
- Logs will show "dkg_threshold was met" for both coordinators
- The divergence only becomes apparent when nodes attempt to use their different keys

**Probability of Success**: HIGH
- Any deployment with `dkg_private_timeout` configured and network jitter > timeout precision
- More likely with aggressive timeouts or geographically distributed nodes
- Guaranteed if timeouts differ between coordinators

### Recommendation

**Primary Fix**: Implement deterministic signer selection that all coordinators agree upon before proceeding.

**Specific Code Changes**:

1. After DkgPublicGather completes, have all coordinators exchange and agree on the complete set of signers who sent DkgPublicShares. Only wait for DkgPrivateShares from this agreed-upon set.

2. When timeout fires, do NOT continue if only dkg_threshold is met. Instead:
   - Broadcast a "DkgPrivateTimeout" message with the set of signers who responded
   - Collect these messages from all coordinators
   - Compute intersection of responded signer sets
   - Only proceed if intersection still meets dkg_threshold
   - All coordinators proceed with the same intersection set

3. Alternative: Remove the early-continuation logic when timeout fires with dkg_threshold met. Only allow two outcomes:
   - All expected signers respond → success
   - Timeout fires without all signers → error (do not continue)

4. Add validation: Before computing aggregate key in `dkg_end_gathered()`, verify that all coordinators are processing the same set of signers (could be done via hash of signer_ids list in DkgEndBegin).

**Testing Recommendations**:
- Add multi-coordinator timeout tests with simulated network delays
- Verify all coordinators compute identical aggregate keys even with timeouts
- Test with minimum threshold met (dkg_threshold exactly) and varying latencies

**Deployment Considerations**:
- This is a breaking change requiring coordinated upgrade
- Consider disabling `dkg_private_timeout` in production until fix is deployed
- Document that coordinators must have synchronized timeout configurations

### Proof of Concept

**Exploitation Steps**:

1. **Setup**: Deploy 2 coordinators (A and B) and 5 signers with these parameters:
   - `num_signers=5, keys_per_signer=2, dkg_threshold=6`
   - Both coordinators configured with `dkg_private_timeout=Duration::from_millis(500)`

2. **Start DKG**: Both coordinators send DkgBegin and successfully complete DkgPublicGather with all 5 signers

3. **Create Timing Differential**: 
   - Coordinators A and B send DkgPrivateBegin
   - Introduce 100ms network delay for signer 4's DkgPrivateShares to coordinator A
   - No delay for signer 4's message to coordinator B

4. **Observe Race Condition**:
   - At T=500ms: Coordinator A's timeout fires
     - Has received shares from signers {0,1,2,3} = 8 keys ≥ 6 threshold
     - Continues to DkgEnd with signers {0,1,2,3}
   - At T=550ms: Signer 4's message arrives at coordinator A (too late)
   - At T=600ms: Coordinator B's timeout fires  
     - Has received shares from signers {0,1,2,3,4} = 10 keys ≥ 6 threshold
     - Continues to DkgEnd with signers {0,1,2,3,4}

5. **Verify Divergence**:
   - Coordinator A computes: `agg_key_A = sum(poly[0] for signers 0,1,2,3)`
   - Coordinator B computes: `agg_key_B = sum(poly[0] for signers 0,1,2,3,4)`
   - Assert: `agg_key_A ≠ agg_key_B`

**Parameter Values**:
- Network delay: 100ms (realistic for cross-region)
- Timeout: 500ms (aggressive but plausible)
- Threshold ratio: 6/10 keys = 60% (typical for fault tolerance)

**Expected vs Actual Behavior**:
- Expected: Both coordinators should compute the same aggregate public key
- Actual: Coordinators compute different keys due to different signer participation

**Reproduction Instructions**:
```
# Modify test to introduce delay between coordinators
# In src/state_machine/coordinator/fire.rs tests:
# Add sleep between coordinator A and B processing messages
# Verify aggregate_public_key differs between coordinators
```

This vulnerability has HIGH severity, practical exploitation, and no effective mitigations in the current codebase.

### Citations

**File:** src/state_machine/coordinator/fire.rs (L105-130)
```rust
            State::DkgPrivateGather => {
                if let Some(start) = self.dkg_private_start {
                    if let Some(timeout) = self.config.dkg_private_timeout {
                        if now.duration_since(start) > timeout {
                            // check dkg_threshold to determine if we can continue
                            let dkg_size = self.compute_dkg_private_size()?;

                            if self.config.dkg_threshold > dkg_size {
                                error!("Timeout gathering DkgPrivateShares for dkg round {} signing round {} iteration {}, dkg_threshold not met ({dkg_size}/{}), unable to continue", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                let wait = self.dkg_wait_signer_ids.iter().copied().collect();
                                return Ok((
                                    None,
                                    Some(OperationResult::DkgError(DkgError::DkgPrivateTimeout(
                                        wait,
                                    ))),
                                ));
                            } else {
                                // we hit the timeout but met the threshold, continue
                                warn!("Timeout gathering DkgPrivateShares for dkg round {} signing round {} iteration {}, dkg_threshold was met ({dkg_size}/{}), ", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                self.private_shares_gathered()?;
                                let packet = self.start_dkg_end()?;
                                return Ok((Some(packet), None));
                            }
                        }
                    }
                }
```

**File:** src/state_machine/coordinator/fire.rs (L449-475)
```rust
    pub fn start_dkg_end(&mut self) -> Result<Packet, Error> {
        // only wait for signers that returned DkgPublicShares
        self.dkg_wait_signer_ids = self
            .dkg_private_shares
            .keys()
            .cloned()
            .collect::<HashSet<u32>>();
        info!(
            dkg_id = %self.current_dkg_id,
            "Starting DkgEnd Distribution"
        );

        let dkg_end_begin = DkgEndBegin {
            dkg_id: self.current_dkg_id,
            signer_ids: self.dkg_private_shares.keys().cloned().collect(),
            key_ids: vec![],
        };
        let dkg_end_begin_msg = Packet {
            sig: dkg_end_begin
                .sign(&self.config.message_private_key)
                .expect("Failed to sign DkgPrivateBegin"),
            msg: Message::DkgEndBegin(dkg_end_begin),
        };
        self.move_to(State::DkgEndGather)?;
        self.dkg_end_start = Some(Instant::now());
        Ok(dkg_end_begin_msg)
    }
```

**File:** src/state_machine/coordinator/fire.rs (L794-812)
```rust
    fn dkg_end_gathered(&mut self) -> Result<(), Error> {
        // Cache the polynomials used in DKG for the aggregator
        for signer_id in self.dkg_private_shares.keys() {
            for (party_id, comm) in &self.dkg_public_shares[signer_id].comms {
                self.party_polynomials.insert(*party_id, comm.clone());
            }
        }

        // Calculate the aggregate public key
        let key = self
            .dkg_end_messages
            .keys()
            .flat_map(|signer_id| self.dkg_public_shares[signer_id].comms.clone())
            .fold(Point::default(), |s, (_, comm)| s + comm.poly[0]);

        info!("Aggregate public key: {key}");
        self.aggregate_public_key = Some(key);
        self.move_to(State::Idle)
    }
```

**File:** src/state_machine/coordinator/fire.rs (L1200-1217)
```rust
    fn compute_num_key_ids<'a, I>(&self, signer_ids: I) -> Result<u32, Error>
    where
        I: Iterator<Item = &'a u32>,
    {
        signer_ids
            .map(
                |signer_id| match self.config.public_keys.signer_key_ids.get(signer_id) {
                    Some(key_ids) => key_ids.len(),
                    None => {
                        error!("No key_ids for signer {signer_id}");
                        0usize
                    }
                },
            )
            .sum::<usize>()
            .try_into()
            .map_err(Error::TryFromInt)
    }
```

**File:** src/state_machine/coordinator/fire.rs (L1223-1225)
```rust
    fn compute_dkg_private_size(&self) -> Result<u32, Error> {
        self.compute_num_key_ids(self.dkg_private_shares.keys())
    }
```
