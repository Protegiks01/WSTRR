### Title
DKG Threshold Default Configuration Causes Denial of Service When Any Signer is Unavailable

### Summary
The `Config::new()` function sets `dkg_threshold` to `num_keys` by default, requiring 100% of all keys to participate in DKG. This causes DKG to fail if any single signer becomes unavailable due to network issues, node failures, or malicious behavior. The failure prevents signature generation and transaction processing, constituting a transient consensus failure with high likelihood in production environments.

### Finding Description

**Exact Code Location:**

The root cause is in `Config::new()` which sets the default `dkg_threshold` to `num_keys`: [1](#0-0) 

**Root Cause Analysis:**

The default configuration requires 100% key participation for DKG to succeed. This is enforced through multiple mechanisms:

1. **FIRE Coordinator Timeout Handling:** When timeouts fire during DKG public or private share gathering, the coordinator computes the number of participating keys and compares it to `dkg_threshold`. If `dkg_threshold > dkg_size`, DKG fails: [2](#0-1) [3](#0-2) 

2. **Signer-Side Validation:** Each signer validates that the number of participating keys meets the `dkg_threshold` requirement. If `num_dkg_keys < dkg_threshold`, the signer returns a `DkgFailure::Threshold` error: [4](#0-3) 

3. **DKG Size Computation:** The coordinator calculates `dkg_size` as the sum of key IDs from signers who responded, not the number of signers: [5](#0-4) 

**Why Existing Mitigations Fail:**

The validation in `Signer::new()` only enforces a lower bound on `dkg_threshold` but no upper bound: [6](#0-5) 

This allows `dkg_threshold = num_keys` to pass validation. The test suite demonstrates the intended usage pattern with 90% threshold, but this is not enforced as a default: [7](#0-6) 

### Impact Explanation

**Specific Harm:**
When `dkg_threshold = num_keys` and any signer is unavailable, DKG cannot complete. Without successful DKG, the system cannot generate the aggregate public key, preventing all subsequent signing operations and transaction processing.

**Quantified Impact:**
- **Without timeouts:** DKG hangs indefinitely waiting for the unavailable signer
- **With timeouts (FIRE only):** DKG fails with `DkgPublicTimeout` or `DkgPrivateTimeout` after timeout expiry
- In a 10-signer, 40-key setup (4 keys per signer), if even 1 signer controlling 4 keys is unavailable, only 36/40 keys participate, failing the 40-key threshold

**Who is Affected:**
Any system integrating WSTS using `Config::new()` without explicitly setting a lower `dkg_threshold` via `Config::with_timeouts()`. This includes production deployments that may assume the default configuration is production-ready.

**Severity Justification:**
This constitutes a **Medium severity** "transient consensus failure" because:
1. DKG failure prevents signature generation
2. No signatures means no transaction confirmation in blockchain contexts
3. The failure is transient (recoverable when signers return or configuration is corrected)
4. It does not cause permanent fund loss or chain splits, but does halt transaction processing

### Likelihood Explanation

**Required Attacker Capabilities:**
- **Passive exploitation:** No attacker neededâ€”natural network issues, node crashes, or maintenance windows trigger the vulnerability
- **Active exploitation:** Attacker needs ability to DoS a single signer (network-level attack, not requiring cryptographic capabilities)

**Attack Complexity:**
- **Low complexity:** Simply preventing one signer from responding to DKG messages is sufficient
- Network partition, packet dropping, or targeted DoS against any single signer triggers failure
- Malicious signer refusing to participate also causes failure

**Economic Feasibility:**
- Minimal cost to attacker: standard network-level DoS techniques
- High impact with minimal effort: disrupting 1 out of N signers halts entire DKG

**Estimated Probability:**
- **Very High** likelihood in production:
  - Network partitions occur regularly
  - Node restarts during maintenance
  - Hardware failures
  - Geographic network issues
  - Buggy or malicious signer implementations

### Recommendation

**Primary Fix:**
Change the default `dkg_threshold` in `Config::new()` to a more reasonable value that allows for signer unavailability while maintaining security. Recommend 90% of `num_keys`:

```rust
pub fn new(
    num_signers: u32,
    num_keys: u32,
    threshold: u32,
    message_private_key: Scalar,
) -> Self {
    Config {
        num_signers,
        num_keys,
        threshold,
        dkg_threshold: (num_keys * 9) / 10,  // Changed from num_keys
        message_private_key,
        // ... rest unchanged
    }
}
```

**Alternative Mitigation:**
Add validation to reject `dkg_threshold` values that equal `num_keys` unless explicitly set via `Config::with_timeouts()`:

```rust
if dkg_threshold == 0 || dkg_threshold < threshold || dkg_threshold > (num_keys * 95) / 100 {
    return Err(Error::Config(ConfigError::InvalidThreshold));
}
```

**Testing Recommendations:**
1. Add integration tests demonstrating DKG success with one signer unavailable when using proper threshold
2. Add negative tests showing DKG failure with 100% threshold and missing signer
3. Document the threshold selection trade-offs in API documentation

**Deployment Considerations:**
- Existing systems using `Config::new()` should migrate to `Config::with_timeouts()` with explicit `dkg_threshold` settings
- Add runtime warnings when `dkg_threshold > 90% of num_keys`
- Consider adding timeout configurations as required parameters rather than optional

### Proof of Concept

**Exploitation Steps:**

1. **Setup:** Initialize coordinator with `Config::new()` (default `dkg_threshold = num_keys`)
2. **Start DKG:** Coordinator broadcasts `DkgBegin` to all signers
3. **Simulate Unavailability:** Prevent one signer from responding (network drop, node down, etc.)
4. **Observe Failure:** 
   - Without timeouts: Coordinator waits forever at `DkgPublicGather` state
   - With timeouts: Coordinator receives responses from N-1 signers, computes `dkg_size < num_keys`, returns `DkgPublicTimeout` error

**Parameter Values:**
- `num_signers = 10`
- `num_keys = 40` (4 keys per signer)
- `threshold = 28` (70% of keys)
- `dkg_threshold = 40` (from `Config::new()` default)
- Unavailable signer controls 4 keys
- Participating keys: 36 < 40 (threshold not met)

**Expected vs Actual Behavior:**
- **Expected:** DKG should succeed with 36/40 keys (90% participation) since this exceeds the signing threshold of 28 keys
- **Actual:** DKG fails because 36 < 40 (dkg_threshold not met)

**Reproduction Instructions:**

Test demonstrating the issue can be derived from existing test setup: [8](#0-7) 

Modify to use `Config::new()` instead of `Config::with_timeouts()`, then simulate one signer not responding during DKG phases. The DKG will fail even though sufficient signers (above the signing threshold) are available.

### Notes

The vulnerability is exacerbated by the FROST coordinator implementation which has no timeout support at all, meaning it will wait indefinitely for all signers: [9](#0-8) [10](#0-9) 

Production systems should use the FIRE coordinator with properly configured timeouts and a realistic `dkg_threshold` below `num_keys`.

### Citations

**File:** src/state_machine/coordinator/mod.rs (L178-200)
```rust
impl Config {
    /// Create a new config object with no timeouts
    pub fn new(
        num_signers: u32,
        num_keys: u32,
        threshold: u32,
        message_private_key: Scalar,
    ) -> Self {
        Config {
            num_signers,
            num_keys,
            threshold,
            dkg_threshold: num_keys,
            message_private_key,
            dkg_public_timeout: None,
            dkg_private_timeout: None,
            dkg_end_timeout: None,
            nonce_timeout: None,
            sign_timeout: None,
            public_keys: Default::default(),
            verify_packet_sigs: true,
        }
    }
```

**File:** src/state_machine/coordinator/mod.rs (L548-561)
```rust
    pub fn setup<Coordinator: CoordinatorTrait, SignerType: SignerTrait>(
        num_signers: u32,
        keys_per_signer: u32,
    ) -> (Vec<Coordinator>, Vec<Signer<SignerType>>) {
        setup_with_timeouts::<Coordinator, SignerType>(
            num_signers,
            keys_per_signer,
            None,
            None,
            None,
            None,
            None,
        )
    }
```

**File:** src/state_machine/coordinator/mod.rs (L579-582)
```rust
        let mut rng = create_rng();
        let num_keys = num_signers * keys_per_signer;
        let threshold = (num_keys * 7) / 10;
        let dkg_threshold = (num_keys * 9) / 10;
```

**File:** src/state_machine/coordinator/fire.rs (L78-99)
```rust
                if let Some(start) = self.dkg_public_start {
                    if let Some(timeout) = self.config.dkg_public_timeout {
                        if now.duration_since(start) > timeout {
                            // check dkg_threshold to determine if we can continue
                            let dkg_size = self.compute_dkg_public_size()?;

                            if self.config.dkg_threshold > dkg_size {
                                error!("Timeout gathering DkgPublicShares for dkg round {} signing round {} iteration {}, dkg_threshold not met ({dkg_size}/{}), unable to continue", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                let wait = self.dkg_wait_signer_ids.iter().copied().collect();
                                return Ok((
                                    None,
                                    Some(OperationResult::DkgError(DkgError::DkgPublicTimeout(
                                        wait,
                                    ))),
                                ));
                            } else {
                                // we hit the timeout but met the threshold, continue
                                warn!("Timeout gathering DkgPublicShares for dkg round {} signing round {} iteration {}, dkg_threshold was met ({dkg_size}/{}), ", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                self.public_shares_gathered()?;
                                let packet = self.start_private_shares()?;
                                return Ok((Some(packet), None));
                            }
```

**File:** src/state_machine/coordinator/fire.rs (L106-127)
```rust
                if let Some(start) = self.dkg_private_start {
                    if let Some(timeout) = self.config.dkg_private_timeout {
                        if now.duration_since(start) > timeout {
                            // check dkg_threshold to determine if we can continue
                            let dkg_size = self.compute_dkg_private_size()?;

                            if self.config.dkg_threshold > dkg_size {
                                error!("Timeout gathering DkgPrivateShares for dkg round {} signing round {} iteration {}, dkg_threshold not met ({dkg_size}/{}), unable to continue", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                let wait = self.dkg_wait_signer_ids.iter().copied().collect();
                                return Ok((
                                    None,
                                    Some(OperationResult::DkgError(DkgError::DkgPrivateTimeout(
                                        wait,
                                    ))),
                                ));
                            } else {
                                // we hit the timeout but met the threshold, continue
                                warn!("Timeout gathering DkgPrivateShares for dkg round {} signing round {} iteration {}, dkg_threshold was met ({dkg_size}/{}), ", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                self.private_shares_gathered()?;
                                let packet = self.start_dkg_end()?;
                                return Ok((Some(packet), None));
                            }
```

**File:** src/state_machine/coordinator/fire.rs (L1219-1225)
```rust
    fn compute_dkg_public_size(&self) -> Result<u32, Error> {
        self.compute_num_key_ids(self.dkg_public_shares.keys())
    }

    fn compute_dkg_private_size(&self) -> Result<u32, Error> {
        self.compute_num_key_ids(self.dkg_private_shares.keys())
    }
```

**File:** src/state_machine/signer/mod.rs (L300-302)
```rust
        if dkg_threshold == 0 || dkg_threshold < threshold {
            return Err(Error::Config(ConfigError::InvalidThreshold));
        }
```

**File:** src/state_machine/signer/mod.rs (L528-549)
```rust
        // fist check to see if dkg_threshold has been met
        let signer_ids_set: HashSet<u32> = dkg_end_begin
            .signer_ids
            .iter()
            .filter(|&&id| id < self.total_signers)
            .copied()
            .collect::<HashSet<u32>>();
        let mut num_dkg_keys = 0u32;
        for id in &signer_ids_set {
            if let Some(key_ids) = self.public_keys.signer_key_ids.get(id) {
                let len: u32 = key_ids.len().try_into()?;
                num_dkg_keys = num_dkg_keys.saturating_add(len);
            }
        }

        if num_dkg_keys < self.dkg_threshold {
            return Ok(Message::DkgEnd(DkgEnd {
                dkg_id: self.dkg_id,
                signer_id: self.signer_id,
                status: DkgStatus::Failure(DkgFailure::Threshold),
            }));
        }
```

**File:** src/state_machine/coordinator/frost.rs (L226-246)
```rust
    pub fn start_public_shares(&mut self) -> Result<Packet, Error> {
        self.dkg_public_shares.clear();
        self.party_polynomials.clear();
        self.ids_to_await = (0..self.config.num_signers).collect();
        info!(
            dkg_id = %self.current_dkg_id,
            "Starting Public Share Distribution"
        );
        let dkg_begin = DkgBegin {
            dkg_id: self.current_dkg_id,
        };

        let dkg_begin_packet = Packet {
            sig: dkg_begin
                .sign(&self.config.message_private_key)
                .expect("Failed to sign DkgBegin"),
            msg: Message::DkgBegin(dkg_begin),
        };
        self.move_to(State::DkgPublicGather)?;
        Ok(dkg_begin_packet)
    }
```

**File:** src/state_machine/coordinator/frost.rs (L290-333)
```rust
    fn gather_public_shares(&mut self, packet: &Packet) -> Result<(), Error> {
        if let Message::DkgPublicShares(dkg_public_shares) = &packet.msg {
            if dkg_public_shares.dkg_id != self.current_dkg_id {
                return Err(Error::BadDkgId(
                    dkg_public_shares.dkg_id,
                    self.current_dkg_id,
                ));
            }

            // check that the signer_id exists in the config
            let signer_public_keys = &self.config.public_keys.signers;
            if !signer_public_keys.contains_key(&dkg_public_shares.signer_id) {
                warn!(signer_id = %dkg_public_shares.signer_id, "No public key in config");
                return Ok(());
            };

            let have_shares = self
                .dkg_public_shares
                .contains_key(&dkg_public_shares.signer_id);

            if have_shares {
                info!(signer_id = %dkg_public_shares.signer_id, "received duplicate DkgPublicShares");
                return Ok(());
            }

            self.ids_to_await.remove(&dkg_public_shares.signer_id);

            self.dkg_public_shares
                .insert(dkg_public_shares.signer_id, dkg_public_shares.clone());
            for (party_id, comm) in &dkg_public_shares.comms {
                self.party_polynomials.insert(*party_id, comm.clone());
            }

            debug!(
                dkg_id = %dkg_public_shares.dkg_id,
                signer_id = %dkg_public_shares.signer_id,
                "DkgPublicShares received"
            );
        }

        if self.ids_to_await.is_empty() {
            self.move_to(State::DkgPrivateDistribute)?;
        }
        Ok(())
```
