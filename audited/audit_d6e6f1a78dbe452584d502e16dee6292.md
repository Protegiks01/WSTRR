### Title
Concurrent Coordinators Cause DKG Consensus Failures via Unprotected Signer State Reset

### Summary
Multiple coordinators with the same configuration can simultaneously initiate DKG rounds, causing signers to repeatedly reset their state and regenerate polynomial commitments with each received `DkgBegin` message. This leads to different coordinators collecting public shares from different polynomial sets, resulting in either failed DKG rounds (denial of service) or different coordinators computing different aggregate public keys (consensus failure).

### Finding Description

**Exact Code Location:**

The vulnerability exists in the signer's `dkg_begin` handler at [1](#0-0) 

This function unconditionally calls `reset()` on every `DkgBegin` message at [2](#0-1) 

The `reset()` function regenerates random polynomials via `reset_polys()` at [3](#0-2) 

New polynomial commitments are generated in `dkg_public_begin()` at [4](#0-3) 

**Root Cause:**

Unlike coordinators which check if a `dkg_id` has already been processed [5](#0-4) , signers have no such protection. The signer blindly resets state for every `DkgBegin` message regardless of whether it has already started processing that `dkg_id`.

**Why Existing Mitigations Fail:**

The test suite explicitly prevents multiple coordinators from sending messages simultaneously by only propagating one coordinator's messages [6](#0-5) . This indicates the scenario of concurrent coordinators was not considered during development.

While coordinators do check for duplicate `dkg_id` values [7](#0-6) , this only prevents a single coordinator from reprocessing its own messages. It does not prevent interference from other coordinators.

### Impact Explanation

**Specific Harm:**

When two coordinators with the same configuration both initiate DKG rounds:

1. **Consensus Failure Scenario:** If both coordinators use the same `dkg_id` (or one synchronizes to the other's ID per [5](#0-4) ), signers regenerate different polynomial sets for each `DkgBegin` received. Coordinator A collects public shares from polynomial set P1, while Coordinator B collects shares from polynomial set P2. They compute different aggregate public keys, violating the DKG invariant that "Group public key must equal the sum of valid polynomial constants."

2. **Denial of Service Scenario:** If coordinators use different `dkg_id` values, signers split between different rounds based on message timing. Neither round achieves sufficient participants (threshold), causing both DKG rounds to fail.

**Quantification:**

- With 2 concurrent coordinators, 100% of DKG attempts fail or produce inconsistent keys
- In blockchain deployments, this prevents new signing key generation or key refresh operations
- If used for multi-signature wallets or blockchain consensus, results in inability to sign transactions or blocks

**Affected Parties:**

All network participants relying on DKG for distributed key generation, including miners, validators, and wallet users in systems like Stacks blockchain.

**Severity Justification:**

This maps to **Medium severity: "Any transient consensus failures"** because different nodes compute different aggregate public keys, causing consensus divergence. It could also be classified as **Low severity: denial of service** when DKG rounds completely fail.

### Likelihood Explanation

**Required Attacker Capabilities:**

- Network access to send messages to signers (standard threat model)
- No cryptographic secrets required
- Can be triggered accidentally by operational mistakes (two coordinators launched simultaneously) or intentionally by any party with network access

**Attack Complexity:**

Low - attacker simply needs to run a second coordinator instance with the same configuration and initiate a DKG round. The coordinator configuration is derived from publicly known network parameters [8](#0-7) .

**Economic Feasibility:**

Minimal cost - requires only the ability to send network messages, which is already possible for any network participant. No stake, bond, or special credentials required.

**Detection Risk:**

Low - appears as normal DKG protocol messages. Difficult to distinguish malicious concurrent coordinator from operational error or network partition.

**Estimated Probability:**

High in production environments where:
- Multiple nodes might independently attempt to coordinate DKG (e.g., during leader election failures)
- An attacker with network access intentionally runs a rogue coordinator
- Operational errors lead to multiple coordinator instances

### Recommendation

**Proposed Code Changes:**

1. Add duplicate `DkgBegin` detection in the signer's `dkg_begin` function before calling `reset()`:

```rust
fn dkg_begin<R: RngCore + CryptoRng>(
    &mut self,
    dkg_begin: &DkgBegin,
    rng: &mut R,
) -> Result<Vec<Message>, Error> {
    // Check if we've already started this DKG round
    if self.dkg_id == dkg_begin.dkg_id && self.state != State::Idle {
        warn!(
            signer_id = %self.signer_id,
            dkg_id = %dkg_begin.dkg_id,
            "Ignoring duplicate DkgBegin message"
        );
        return Ok(vec![]);
    }
    
    self.reset(dkg_begin.dkg_id, rng);
    self.move_to(State::DkgPublicDistribute)?;
    self.dkg_public_begin(rng)
}
```

2. Add coordinator authentication to ensure only one authorized coordinator can initiate rounds. Leverage the existing `verify_packet_sigs` mechanism and `coordinator_public_key` field [9](#0-8)  to validate that DKG initiation messages come from the designated coordinator.

**Alternative Mitigations:**

- Implement leader election protocol to ensure only one coordinator is active at a time
- Add round ID monotonicity checks to reject old round IDs (similar to coordinator behavior)
- Implement coordinator discovery/registration to prevent unauthorized coordinators

**Testing Recommendations:**

Add test cases for:
- Multiple coordinators with same `dkg_id` sending `DkgBegin` messages
- Multiple coordinators with different `dkg_id` values
- Verify signers ignore duplicate `DkgBegin` for same `dkg_id`
- Verify different coordinators cannot interfere with each other's rounds

**Deployment Considerations:**

- Requires coordinated update of all signers before deploying multiple coordinator support
- May require configuration changes to designate authorized coordinator(s)
- Should be combined with monitoring to detect unauthorized coordinator activity

### Proof of Concept

**Exploitation Algorithm:**

1. Deploy two coordinator instances (A and B) with identical `Config` parameters [8](#0-7) 

2. Both coordinators call `start_dkg_round(Some(1))` simultaneously, each generating a `DkgBegin` message with `dkg_id = 1` [10](#0-9) 

3. Signers receive both `DkgBegin(dkg_id=1)` messages in rapid succession

4. First `DkgBegin` triggers signer to reset and generate polynomial set P1 [2](#0-1) , then send `DkgPublicShares` to Coordinator A

5. Second `DkgBegin` triggers signer to reset AGAIN and generate polynomial set P2 (different random polynomials), then send `DkgPublicShares` to Coordinator B

6. Coordinator A aggregates public shares from polynomial set P1, computing aggregate key K1 [11](#0-10) 

7. Coordinator B aggregates public shares from polynomial set P2, computing aggregate key K2 â‰  K1

**Expected vs Actual Behavior:**

- **Expected:** Both coordinators should agree on the same aggregate public key, or only one coordinator should successfully complete DKG
- **Actual:** Different coordinators compute different aggregate public keys, or both DKG rounds fail due to insufficient/inconsistent shares

**Reproduction Steps:**

Using the test framework [12](#0-11) :

1. Create 2 coordinators and 5 signers with `setup::<Coordinator, SignerType>(5, 2)`
2. Have both coordinators call `start_dkg_round(Some(1))`  
3. Feed both `DkgBegin` messages to all signers via `process()`
4. Observe that signers send different `DkgPublicShares` to each coordinator
5. Continue DKG protocol and observe different aggregate keys or failed DKG

### Citations

**File:** src/state_machine/signer/mod.rs (L163-164)
```rust
    /// coordinator public key
    pub coordinator_public_key: Option<ecdsa::PublicKey>,
```

**File:** src/state_machine/signer/mod.rs (L417-432)
```rust
    pub fn reset<T: RngCore + CryptoRng>(&mut self, dkg_id: u64, rng: &mut T) {
        self.dkg_id = dkg_id;
        self.commitments.clear();
        self.decrypted_shares.clear();
        self.decryption_keys.clear();
        self.invalid_private_shares.clear();
        self.public_nonces.clear();
        self.signer.reset_polys(rng);
        self.dkg_public_shares.clear();
        self.dkg_private_shares.clear();
        self.dkg_private_begin_msg = None;
        self.dkg_end_begin_msg = None;
        self.kex_private_key = Scalar::random(rng);
        self.kex_public_keys.clear();
        self.state = State::Idle;
    }
```

**File:** src/state_machine/signer/mod.rs (L844-855)
```rust
    fn dkg_begin<R: RngCore + CryptoRng>(
        &mut self,
        dkg_begin: &DkgBegin,
        rng: &mut R,
    ) -> Result<Vec<Message>, Error> {
        self.reset(dkg_begin.dkg_id, rng);
        self.move_to(State::DkgPublicDistribute)?;

        //let _party_state = self.signer.save();

        self.dkg_public_begin(rng)
    }
```

**File:** src/state_machine/signer/mod.rs (L857-890)
```rust
    fn dkg_public_begin<R: RngCore + CryptoRng>(
        &mut self,
        rng: &mut R,
    ) -> Result<Vec<Message>, Error> {
        let mut msgs = vec![];
        let comms = self
            .signer
            .get_poly_commitments(&self.dkg_id.to_be_bytes(), rng);

        info!(
            signer_id = %self.signer_id,
            dkg_id = %self.dkg_id,
            "sending DkgPublicShares"
        );

        let mut public_share = DkgPublicShares {
            dkg_id: self.dkg_id,
            signer_id: self.signer_id,
            comms: Vec::new(),
            kex_public_key: self.kex_private_key * G,
        };

        for poly in &comms {
            public_share
                .comms
                .push((poly.id.id.get_u32(), poly.clone()));
        }

        let public_share = Message::DkgPublicShares(public_share);
        msgs.push(public_share);

        self.move_to(State::DkgPublicGather)?;
        Ok(msgs)
    }
```

**File:** src/state_machine/coordinator/frost.rs (L75-82)
```rust
                    if let Message::DkgBegin(dkg_begin) = &packet.msg {
                        if self.current_dkg_id == dkg_begin.dkg_id {
                            // We have already processed this DKG round
                            return Ok((None, None));
                        }
                        // use dkg_id from DkgBegin
                        let packet = self.start_dkg_round(Some(dkg_begin.dkg_id))?;
                        return Ok((Some(packet), None));
```

**File:** src/state_machine/coordinator/frost.rs (L422-446)
```rust
    fn dkg_end_gathered(&mut self) -> Result<(), Error> {
        // Cache the polynomials used in DKG for the aggregator
        for signer_id in self.dkg_private_shares.keys() {
            let Some(dkg_public_shares) = self.dkg_public_shares.get(signer_id) else {
                warn!(%signer_id, "no DkgPublicShares");
                return Err(Error::BadStateChange(format!("Should not have transitioned to DkgEndGather since we were missing DkgPublicShares from signer {signer_id}")));
            };
            for (party_id, comm) in &dkg_public_shares.comms {
                self.party_polynomials.insert(*party_id, comm.clone());
            }
        }

        // Calculate the aggregate public key
        let key = self
            .party_polynomials
            .iter()
            .fold(Point::default(), |s, (_, comm)| s + comm.poly[0]);

        info!(
            %key,
            "Aggregate public key"
        );
        self.aggregate_public_key = Some(key);
        self.move_to(State::Idle)
    }
```

**File:** src/state_machine/coordinator/frost.rs (L957-966)
```rust
    fn start_dkg_round(&mut self, dkg_id: Option<u64>) -> Result<Packet, Error> {
        if let Some(id) = dkg_id {
            self.current_dkg_id = id;
        } else {
            self.current_dkg_id = self.current_dkg_id.wrapping_add(1);
        }
        info!("Starting DKG round {}", self.current_dkg_id);
        self.move_to(State::DkgPublicDistribute)?;
        self.start_public_shares()
    }
```

**File:** src/state_machine/coordinator/frost.rs (L1507-1575)
```rust
    fn old_round_ids_are_ignored<Aggregator: AggregatorTrait>() {
        let mut rng = create_rng();
        let mut config = Config::new(10, 40, 28, Scalar::random(&mut rng));
        config.verify_packet_sigs = false;
        let mut coordinator = FrostCoordinator::<Aggregator>::new(config);
        let id: u64 = 10;
        let old_id = id;
        coordinator.current_dkg_id = id;
        coordinator.current_sign_id = id;
        // Attempt to start an old DKG round
        let (packet, result) = coordinator
            .process(&Packet {
                sig: vec![],
                msg: Message::DkgBegin(DkgBegin { dkg_id: old_id }),
            })
            .unwrap();
        assert!(packet.is_none());
        assert!(result.is_none());
        assert_eq!(coordinator.state, State::Idle);
        assert_eq!(coordinator.current_dkg_id, id);

        // Attempt to start the same DKG round
        let (packet, result) = coordinator
            .process(&Packet {
                sig: vec![],
                msg: Message::DkgBegin(DkgBegin { dkg_id: id }),
            })
            .unwrap();
        assert!(packet.is_none());
        assert!(result.is_none());
        assert_eq!(coordinator.state, State::Idle);
        assert_eq!(coordinator.current_dkg_id, id);

        // Attempt to start an old Sign round
        let (packet, result) = coordinator
            .process(&Packet {
                sig: vec![],
                msg: Message::NonceRequest(NonceRequest {
                    dkg_id: id,
                    sign_id: old_id,
                    message: vec![],
                    sign_iter_id: id,
                    signature_type: SignatureType::Frost,
                }),
            })
            .unwrap();
        assert!(packet.is_none());
        assert!(result.is_none());
        assert_eq!(coordinator.state, State::Idle);
        assert_eq!(coordinator.current_sign_id, id);

        // Attempt to start the same Sign round
        let (packet, result) = coordinator
            .process(&Packet {
                sig: vec![],
                msg: Message::NonceRequest(NonceRequest {
                    dkg_id: id,
                    sign_id: id,
                    message: vec![],
                    sign_iter_id: id,
                    signature_type: SignatureType::Frost,
                }),
            })
            .unwrap();
        assert!(packet.is_none());
        assert!(result.is_none());
        assert_eq!(coordinator.state, State::Idle);
        assert_eq!(coordinator.current_sign_id, id);
    }
```

**File:** src/state_machine/coordinator/mod.rs (L132-158)
```rust
#[derive(Default, Clone, PartialEq)]
pub struct Config {
    /// total number of signers
    pub num_signers: u32,
    /// total number of keys
    pub num_keys: u32,
    /// threshold of keys needed to form a valid signature
    pub threshold: u32,
    /// threshold of keys needed to complete DKG (must be >= threshold)
    pub dkg_threshold: u32,
    /// private key used to sign network messages
    pub message_private_key: Scalar,
    /// timeout to gather DkgPublicShares messages
    pub dkg_public_timeout: Option<Duration>,
    /// timeout to gather DkgPrivateShares messages
    pub dkg_private_timeout: Option<Duration>,
    /// timeout to gather DkgEnd messages
    pub dkg_end_timeout: Option<Duration>,
    /// timeout to gather nonces
    pub nonce_timeout: Option<Duration>,
    /// timeout to gather signature shares
    pub sign_timeout: Option<Duration>,
    /// the public keys and key_ids for all signers
    pub public_keys: PublicKeys,
    /// whether to verify the signature on Packets
    pub verify_packet_sigs: bool,
}
```

**File:** src/state_machine/coordinator/mod.rs (L548-657)
```rust
    pub fn setup<Coordinator: CoordinatorTrait, SignerType: SignerTrait>(
        num_signers: u32,
        keys_per_signer: u32,
    ) -> (Vec<Coordinator>, Vec<Signer<SignerType>>) {
        setup_with_timeouts::<Coordinator, SignerType>(
            num_signers,
            keys_per_signer,
            None,
            None,
            None,
            None,
            None,
        )
    }

    pub fn setup_with_timeouts<Coordinator: CoordinatorTrait, SignerType: SignerTrait>(
        num_signers: u32,
        keys_per_signer: u32,
        dkg_public_timeout: Option<Duration>,
        dkg_private_timeout: Option<Duration>,
        dkg_end_timeout: Option<Duration>,
        nonce_timeout: Option<Duration>,
        sign_timeout: Option<Duration>,
    ) -> (Vec<Coordinator>, Vec<Signer<SignerType>>) {
        INIT.call_once(|| {
            tracing_subscriber::registry()
                .with(fmt::layer())
                .with(EnvFilter::from_default_env())
                .init();
        });

        let mut rng = create_rng();
        let num_keys = num_signers * keys_per_signer;
        let threshold = (num_keys * 7) / 10;
        let dkg_threshold = (num_keys * 9) / 10;
        let key_pairs = (0..num_signers)
            .map(|_| {
                let private_key = Scalar::random(&mut rng);
                let public_key = ecdsa::PublicKey::new(&private_key).unwrap();
                (private_key, public_key)
            })
            .collect::<Vec<(Scalar, ecdsa::PublicKey)>>();
        let mut key_id: u32 = 1;
        let mut signer_ids_map = HashMap::new();
        let mut signer_key_ids = HashMap::new();
        let mut signer_key_ids_set = HashMap::new();
        let mut signer_public_keys = HashMap::new();
        let mut key_ids_map = HashMap::new();
        for (i, (private_key, public_key)) in key_pairs.iter().enumerate() {
            let mut key_ids = Vec::new();
            let mut key_ids_set = HashSet::new();
            for _ in 0..keys_per_signer {
                key_ids_map.insert(key_id, *public_key);
                key_ids.push(key_id);
                key_ids_set.insert(key_id);
                key_id += 1;
            }
            signer_ids_map.insert(i as u32, *public_key);
            signer_key_ids.insert(i as u32, key_ids);
            signer_key_ids_set.insert(i as u32, key_ids_set);
            signer_public_keys.insert(i as u32, Point::from(private_key));
        }
        let public_keys = PublicKeys {
            signers: signer_ids_map,
            key_ids: key_ids_map,
            signer_key_ids: signer_key_ids_set.clone(),
        };

        let signers = key_pairs
            .iter()
            .enumerate()
            .map(|(signer_id, (private_key, _public_key))| {
                let mut signer = Signer::<SignerType>::new(
                    threshold,
                    dkg_threshold,
                    num_signers,
                    num_keys,
                    signer_id as u32,
                    signer_key_ids[&(signer_id as u32)].clone(),
                    *private_key,
                    public_keys.clone(),
                    &mut rng,
                )
                .unwrap();
                signer.verify_packet_sigs = false;
                signer
            })
            .collect::<Vec<Signer<SignerType>>>();
        let coordinators = key_pairs
            .into_iter()
            .map(|(private_key, _public_key)| {
                let mut config = Config::with_timeouts(
                    num_signers,
                    num_keys,
                    threshold,
                    dkg_threshold,
                    private_key,
                    dkg_public_timeout,
                    dkg_private_timeout,
                    dkg_end_timeout,
                    nonce_timeout,
                    sign_timeout,
                    public_keys.clone(),
                );
                config.verify_packet_sigs = false;
                Coordinator::new(config)
            })
            .collect::<Vec<Coordinator>>();
        (coordinators, signers)
    }
```

**File:** src/state_machine/coordinator/mod.rs (L717-734)
```rust
        for coordinator in coordinators.iter_mut() {
            // Process all coordinator messages, but don't bother with propogating these results
            for message in messages {
                let _ = coordinator.process(message)?;
            }
        }
        let mut results = vec![];
        let mut messages = vec![];
        for (i, coordinator) in coordinators.iter_mut().enumerate() {
            for inbound_message in &inbound_messages {
                let (outbound_message, outbound_result) = coordinator.process(inbound_message)?;
                // Only propogate a single coordinator's messages and results
                if i == 0 {
                    messages.extend(outbound_message);
                    results.extend(outbound_result);
                }
            }
        }
```
