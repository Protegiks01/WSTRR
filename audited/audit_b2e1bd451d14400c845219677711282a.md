### Title
FROST Coordinator Silently Ignores Timeout Configuration, Enabling Denial of Service

### Summary
The FROST coordinator implementation violates its documented trait contract by failing to check timeouts before processing packets. While the Coordinator trait explicitly requires timeout checking with stale packet rejection, FROST directly processes all packets without timeout enforcement, creating a denial-of-service vulnerability where a single unresponsive signer can indefinitely block DKG or signing rounds.

### Finding Description

The Coordinator trait documents that `process()` must check timeouts before processing packets: [1](#0-0) 

However, the FROST coordinator's implementation violates this contract by directly calling `process_message(packet)` without any timeout checking: [2](#0-1) 

In contrast, the FIRE coordinator correctly implements the contract by checking timeouts first: [3](#0-2) 

**Root Cause:** The FROST coordinator was designed as a non-robust version for trusted environments and lacks the timeout tracking infrastructure (fields like `nonce_start`, `dkg_public_start`, `sign_start`, etc.) that FIRE implements: [4](#0-3) 

However, both coordinators use the same Config struct that includes timeout fields: [5](#0-4) 

**Why Existing Mitigations Fail:**
- No documentation warns against using FROST with timeouts configured
- No type-system enforcement prevents timeout configuration for FROST
- Timeouts can be set via `Config::with_timeouts()` for both coordinators
- Silent failure mode: timeouts are accepted but ignored

### Impact Explanation

**Specific Harm:**
A malicious or unresponsive signer can block protocol execution indefinitely:
- During DKG: Block aggregate public key generation by not sending DkgPublicShares, DkgPrivateShares, or DkgEnd messages
- During Signing: Block signature generation by not sending NonceResponse or SignatureShareResponse messages

**Quantified Impact:**
- Single unresponsive signer can DoS the entire coordinator
- All pending signature requests blocked until manual intervention
- Transactions requiring threshold signatures cannot be confirmed
- No automatic malicious signer detection or exclusion (unlike FIRE)

**Who is Affected:**
Systems deploying FROST coordinator with timeout configurations for DoS protection, falsely believing timeouts provide resilience.

**Severity Justification:**
This maps to **Low** severity per the audit scope: "Any remotely-exploitable denial of service in a node." The attack blocks signature generation, preventing transaction confirmation, but does not directly cause fund loss or consensus failures.

### Likelihood Explanation

**Required Attacker Capabilities:**
- Participation as a legitimate signer in the protocol
- Ability to stop responding at critical protocol phases
- No cryptographic capabilities required

**Attack Complexity:**
Low - attacker simply stops responding:
1. Join DKG or signing round as authorized signer
2. Participate normally through initial phases
3. Stop sending messages during critical gathering phase
4. Coordinator waits indefinitely (no timeout enforcement)
5. Protocol execution blocked

**Economic Feasibility:**
Highly feasible - requires minimal resources, just network participation.

**Detection Risk:**
Low detection risk initially, as unresponsiveness could be attributed to network issues. However, repeated patterns would become suspicious.

**Estimated Probability:**
- **High** if FROST is deployed with timeouts configured (silent failure creates false security)
- **Medium** if FROST is deployed in untrusted environments without understanding its limitations
- **Low** if FROST is only used in fully trusted environments as intended

### Recommendation

**Primary Fix - Implement Timeout Enforcement in FROST:**
Add timeout tracking fields and implement `process_timeout()` in FROST coordinator:

1. Add timeout tracking fields to FROST Coordinator struct (matching FIRE's implementation)
2. Implement `process_timeout()` method to check and enforce configured timeouts
3. Modify `process()` to call `process_timeout()` before `process_message()`
4. Set timeout start times when transitioning to gathering states

**Alternative Mitigation - Documentation and Type Safety:**
If FROST is intended to remain non-robust:

1. Document clearly that FROST does not support timeouts and is only for trusted environments
2. Create separate Config types: `FrostConfig` (no timeout fields) and `FireConfig` (with timeouts)
3. Add compile-time enforcement preventing timeout configuration for FROST
4. Add runtime warning if timeouts are configured for FROST

**Testing Recommendations:**
1. Add test verifying FROST with configured timeouts either enforces them or explicitly rejects the configuration
2. Add integration test demonstrating DoS scenario with unresponsive signer
3. Verify timeout enforcement doesn't break normal protocol flow

**Deployment Considerations:**
- Audit existing FROST deployments for timeout configurations
- Migrate to FIRE coordinator for production environments requiring DoS resilience
- Document coordinator selection criteria based on trust model

### Proof of Concept

**Exploitation Steps:**

1. Configure FROST coordinator with timeouts (falsely expecting DoS protection):
```rust
// System deploys FROST with timeouts, expecting protection
let config = Config::with_timeouts(
    num_signers: 10,
    num_keys: 40, 
    threshold: 28,
    dkg_threshold: 36,
    message_private_key,
    dkg_public_timeout: Some(Duration::from_secs(30)),
    dkg_private_timeout: Some(Duration::from_secs(30)),
    dkg_end_timeout: Some(Duration::from_secs(30)),
    nonce_timeout: Some(Duration::from_secs(30)),
    sign_timeout: Some(Duration::from_secs(30)),
    public_keys
);
let mut coordinator = FrostCoordinator::<Aggregator>::new(config);
```

2. Start signing round:
```rust
coordinator.start_signing_round(&message, SignatureType::Frost, None)?;
// State: NonceGather
```

3. Honest signers respond with nonces, malicious signer (ID 9) does not respond.

4. Coordinator waits in NonceGather state, calling `process()` for each packet.

5. Expected behavior (with timeout enforcement): After 30 seconds, timeout fires, malicious signer excluded, signing continues with remaining signers.

6. Actual behavior (FROST): Coordinator waits indefinitely, never checks timeout, protocol hangs.

**Expected vs Actual Behavior:**
- Expected: `process_timeout()` returns error after 30 seconds, protocol can recover
- Actual: FROST has no `process_timeout()` implementation, `process()` only calls `process_message()`, timeout configuration silently ignored

**Reproduction:**
See test code structure in FIRE coordinator tests for timeout scenarios: [6](#0-5) 

Adapt this test for FROST coordinator - it will demonstrate that the timeout never fires and protocol hangs indefinitely.

### Citations

**File:** src/state_machine/coordinator/mod.rs (L144-157)
```rust
    /// timeout to gather DkgPublicShares messages
    pub dkg_public_timeout: Option<Duration>,
    /// timeout to gather DkgPrivateShares messages
    pub dkg_private_timeout: Option<Duration>,
    /// timeout to gather DkgEnd messages
    pub dkg_end_timeout: Option<Duration>,
    /// timeout to gather nonces
    pub nonce_timeout: Option<Duration>,
    /// timeout to gather signature shares
    pub sign_timeout: Option<Duration>,
    /// the public keys and key_ids for all signers
    pub public_keys: PublicKeys,
    /// whether to verify the signature on Packets
    pub verify_packet_sigs: bool,
```

**File:** src/state_machine/coordinator/mod.rs (L328-334)
```rust
    /// Process any timeouts, and if none of them fire then process the passed packet
    /// If a timeout does fire, then the coordinator state has changed; this means the
    /// packet is now stale and must be dropped
    fn process(
        &mut self,
        packet: &Packet,
    ) -> Result<(Option<Packet>, Option<OperationResult>), Error>;
```

**File:** src/state_machine/coordinator/frost.rs (L929-934)
```rust
    fn process(
        &mut self,
        packet: &Packet,
    ) -> Result<(Option<Packet>, Option<OperationResult>), Error> {
        self.process_message(packet)
    }
```

**File:** src/state_machine/coordinator/fire.rs (L59-63)
```rust
    nonce_start: Option<Instant>,
    dkg_public_start: Option<Instant>,
    dkg_private_start: Option<Instant>,
    dkg_end_start: Option<Instant>,
    sign_start: Option<Instant>,
```

**File:** src/state_machine/coordinator/fire.rs (L1441-1454)
```rust
    /// Process the timeouts, and if none of them fire then process the passed packet
    /// If a timeout does fire, then the coordinator state has changed; this means the
    /// packet is now stale and must be dropped
    fn process(
        &mut self,
        packet: &Packet,
    ) -> Result<(Option<Packet>, Option<OperationResult>), Error> {
        let (outbound_packet, operation_result) = self.process_timeout()?;
        if outbound_packet.is_some() || operation_result.is_some() {
            return Ok((outbound_packet, operation_result));
        }

        self.process_message(packet)
    }
```

**File:** src/state_machine/coordinator/fire.rs (L2077-2150)
```rust
    fn missing_public_keys_dkg<Aggregator: AggregatorTrait, SignerType: SignerTrait>(
        num_signers: u32,
        keys_per_signer: u32,
    ) -> (Vec<FireCoordinator<Aggregator>>, Vec<Signer<SignerType>>) {
        let timeout = Duration::from_millis(1024);
        let expire = Duration::from_millis(1280);
        let (mut coordinators, signers) =
            setup_with_timeouts::<FireCoordinator<Aggregator>, SignerType>(
                num_signers,
                keys_per_signer,
                Some(timeout),
                Some(timeout),
                Some(timeout),
                Some(timeout),
                Some(timeout),
            );

        // Start a DKG round where we will not allow all signers to recv DkgBegin, so they will not respond with DkgPublicShares
        let message = coordinators
            .first_mut()
            .unwrap()
            .start_dkg_round(None)
            .unwrap();
        assert!(coordinators.first().unwrap().aggregate_public_key.is_none());
        assert_eq!(coordinators.first().unwrap().state, State::DkgPublicGather);

        let mut minimum_coordinators = coordinators.clone();
        let mut minimum_signers = signers.clone();

        // Let us also remove that signers public key from the config including all of its key ids
        let mut removed_signer = minimum_signers.pop().expect("Failed to pop signer");
        let public_key = removed_signer
            .public_keys
            .signers
            .remove(&removed_signer.signer_id)
            .expect("Failed to remove public key");
        removed_signer
            .public_keys
            .key_ids
            .retain(|_k, pk| pk.to_bytes() != public_key.to_bytes());

        for signer in minimum_signers.iter_mut() {
            // Overwrite all other signers to use the new public keys missing the removed signers public key
            signer.public_keys = removed_signer.public_keys.clone();
        }

        // Send the DKG Begin message to minimum signers and gather responses by sharing with signers and coordinator
        let (outbound_messages, operation_results) = feedback_messages(
            &mut minimum_coordinators,
            &mut minimum_signers,
            from_ref(&message),
        );

        assert!(outbound_messages.is_empty());
        assert!(operation_results.is_empty());
        assert_eq!(coordinators.first().unwrap().state, State::DkgPublicGather,);

        // Sleep long enough to hit the timeout
        thread::sleep(expire);

        let (outbound_messages, operation_results) = minimum_coordinators
            .first_mut()
            .unwrap()
            .process_timeout()
            .unwrap();

        assert!(outbound_messages.is_some());
        assert!(operation_results.is_none());
        assert_eq!(
            minimum_coordinators.first().unwrap().state,
            State::DkgPrivateGather,
        );
        (minimum_coordinators, minimum_signers)
    }
```
