### Title
Coordinator Panic via Signature Share Validation Failure During Aggregation

### Summary
The FIRE coordinator's `gather_sig_shares()` function removes signers from the wait list before validating their signature shares. If validation fails, the signer remains in `public_nonces` but is absent from `signature_shares`. When aggregation occurs, unsafe indexing into `signature_shares` using keys from `public_nonces` causes a panic, crashing the coordinator.

### Finding Description

**Location**: `src/state_machine/coordinator/fire.rs`, function `gather_sig_shares()`, lines 998-1174

**Root Cause**: The function exhibits a critical ordering flaw in its validation logic:

1. When a `SignatureShareResponse` arrives, the signer is immediately removed from `sign_wait_signer_ids` [1](#0-0) 

2. After removal, multiple validation checks are performed that can fail and return early with errors [2](#0-1) 

3. Only if all validations pass are the shares inserted into `signature_shares` [3](#0-2) 

4. When validation fails, the error is caught at the caller level and converted to a `SignError`, but the coordinator state remains inconsistent [4](#0-3) 

5. The signer who provided valid nonces earlier remains in `public_nonces` (populated during nonce gathering) [5](#0-4)  but is absent from `signature_shares`

6. When `sign_wait_signer_ids` becomes empty after other signers respond, the aggregation code executes and uses unsafe indexing: `self.signature_shares[i]` where `i` comes from iterating `public_nonces` [6](#0-5) 

**Why Existing Mitigations Fail**: 
- No code exists to remove failed signers from `public_nonces` (verified via grep, no `public_nonces.remove()` calls exist)
- The indexing operation uses the unsafe `[]` operator instead of `.get()` or other safe accessors
- The check `sign_wait_signer_ids.is_empty()` [7](#0-6)  assumes all signers in `public_nonces` have corresponding entries in `signature_shares`, but this invariant is violated when validation fails

### Impact Explanation

**Specific Harm**: The coordinator panics (crashes via index out of bounds) when attempting signature aggregation. This is a remotely-exploitable denial of service.

**Quantification**: 
- A single malicious signer can crash the coordinator during any signing round
- The coordinator must be restarted and the signing round retried
- During the crash, no signatures can be produced, preventing transaction confirmations
- In WSTS-based Stacks deployments, this blocks mining and transaction processing

**Affected Parties**:
- All participants depending on the coordinator for threshold signatures
- In blockchain contexts: miners unable to sign blocks, users unable to confirm transactions

**Severity Justification**: This maps to **Low severity** under the protocol scope definition: "Any remotely-exploitable denial of service in a node." However, if the coordinator crash causes transient consensus failures or affects more than 10% of miners (depending on deployment architecture), this could escalate to **Medium severity**.

### Likelihood Explanation

**Required Attacker Capabilities**:
- Must be a valid signer in the configured signer set (have a `signer_id` in `config.public_keys.signers`)
- Must have network access to send messages to the coordinator
- Must participate in nonce gathering (send valid `NonceResponse`)

**Attack Complexity**: LOW
1. Attacker provides valid nonces during nonce gathering phase (gets added to `public_nonces` and `sign_wait_signer_ids`) [8](#0-7) 
2. When signature shares are requested, `signature_shares` is cleared [9](#0-8) 
3. Attacker sends a `SignatureShareResponse` with invalid data (e.g., `key_ids` that don't match the configured `signer_key_ids`)
4. Validation fails at the key_ids check [10](#0-9) 
5. Function returns error, attacker is removed from wait list but NOT added to `signature_shares`
6. Honest signers provide valid responses
7. When wait list empties, aggregation code panics on `self.signature_shares[attacker_id]`

**Economic Feasibility**: Trivial. No financial cost beyond network participation.

**Detection Risk**: Low. The malformed response appears as a normal validation failure until the panic occurs during aggregation.

**Probability of Success**: Near 100%. The attack is deterministic once the attacker is a valid signer.

### Recommendation

**Primary Fix**: Use safe indexing with proper error handling in the aggregation code:

```rust
let shares = message_nonce
    .public_nonces
    .iter()
    .filter_map(|(i, _)| self.signature_shares.get(i).cloned())
    .flatten()
    .collect::<Vec<SignatureShare>>();
```

This changes `flat_map` with unsafe indexing to `filter_map` with `.get()`, silently skipping signers without shares.

**Alternative Mitigation 1**: Remove failed signers from `public_nonces` when validation fails:

```rust
// After line 1053, add:
response_info.public_nonces.remove(&sig_share_response.signer_id);
```

Add similar cleanup at each validation error return point (lines 1050, 1063, 1075).

**Alternative Mitigation 2**: Reorder validation to occur before removing from wait list. Move lines 1042-1044 to after line 1091 (after successful insertion into `signature_shares`).

**Testing Recommendations**:
- Add unit test with a signer providing valid nonces but invalid signature shares
- Verify coordinator doesn't panic and either produces signature with remaining shares or returns appropriate error
- Test with threshold boundary cases (e.g., threshold signers respond validly, one responds invalidly)

**Deployment Considerations**: This fix should be backported to all deployed WSTS versions. Coordinators should be updated before signers to prevent disruption.

### Proof of Concept

**Exploitation Algorithm**:

1. **Setup**: Deploy WSTS with threshold=2, 3 signers (IDs: 0, 1, 2). Attacker controls signer 2.

2. **Nonce Phase**: 
   - All signers send valid `NonceResponse` messages
   - Coordinator adds all to `public_nonces` and `sign_wait_signer_ids`

3. **Signature Share Phase**:
   - Coordinator sends `SignatureShareRequest` 
   - Coordinator clears `signature_shares`
   - Signer 0 sends valid `SignatureShareResponse` (added to `signature_shares`)
   - Attacker (signer 2) sends `SignatureShareResponse` with mismatched `key_ids`:
     * Config expects `key_ids: [5, 6]` for signer 2
     * Attacker sends `key_ids: [7, 8]`
   - Validation fails at line 1073-1076, returns `Error::BadKeyIDsForSigner(2)`
   - Signer 2 removed from `sign_wait_signer_ids` but NOT added to `signature_shares`
   - Signer 1 sends valid `SignatureShareResponse` (added to `signature_shares`)
   - `sign_wait_signer_ids` now empty (all three signers processed)

4. **Aggregation Triggers**:
   - Line 1113 condition `sign_wait_signer_ids.is_empty()` is true
   - Lines 1131-1135 execute
   - Iterator over `public_nonces` includes keys: [0, 1, 2]
   - Indexing `self.signature_shares[0]`: exists ✓
   - Indexing `self.signature_shares[1]`: exists ✓
   - Indexing `self.signature_shares[2]`: **PANIC** - key not found

**Expected vs Actual Behavior**:
- Expected: Coordinator either (a) aggregates with remaining valid shares if threshold met, or (b) returns error indicating insufficient valid shares
- Actual: Coordinator panics with index out of bounds error, crashes

**Reproduction**: Create a modified signer client that sends valid nonces but invalid signature shares as described above, then observe coordinator panic during aggregation.

### Citations

**File:** src/state_machine/coordinator/fire.rs (L328-333)
```rust
                    if let Err(e) = self.gather_sig_shares(packet, signature_type) {
                        return Ok((
                            None,
                            Some(OperationResult::SignError(SignError::Coordinator(e))),
                        ));
                    }
```

**File:** src/state_machine/coordinator/fire.rs (L931-933)
```rust
            nonce_info
                .public_nonces
                .insert(nonce_response.signer_id, nonce_response.clone());
```

**File:** src/state_machine/coordinator/fire.rs (L940-942)
```rust
            nonce_info
                .sign_wait_signer_ids
                .insert(nonce_response.signer_id);
```

**File:** src/state_machine/coordinator/fire.rs (L965-965)
```rust
        self.signature_shares.clear();
```

**File:** src/state_machine/coordinator/fire.rs (L1042-1044)
```rust
        response_info
            .sign_wait_signer_ids
            .remove(&sig_share_response.signer_id);
```

**File:** src/state_machine/coordinator/fire.rs (L1046-1076)
```rust
        // check that the signer_id exists in the config
        let signer_public_keys = &self.config.public_keys.signers;
        if !signer_public_keys.contains_key(&sig_share_response.signer_id) {
            warn!(signer_id = %sig_share_response.signer_id, "No public key in config");
            return Err(Error::MissingPublicKeyForSigner(
                sig_share_response.signer_id,
            ));
        };

        // check that the key_ids match the config
        let Some(signer_key_ids) = self
            .config
            .public_keys
            .signer_key_ids
            .get(&sig_share_response.signer_id)
        else {
            warn!(signer_id = %sig_share_response.signer_id, "No keys IDs configured");
            return Err(Error::MissingKeyIDsForSigner(sig_share_response.signer_id));
        };

        let mut sig_share_response_key_ids = HashSet::new();
        for sig_share in &sig_share_response.signature_shares {
            for key_id in &sig_share.key_ids {
                sig_share_response_key_ids.insert(*key_id);
            }
        }

        if *signer_key_ids != sig_share_response_key_ids {
            warn!(signer_id = %sig_share_response.signer_id, "SignatureShareResponse key_ids didn't match config");
            return Err(Error::BadKeyIDsForSigner(sig_share_response.signer_id));
        }
```

**File:** src/state_machine/coordinator/fire.rs (L1088-1091)
```rust
        self.signature_shares.insert(
            sig_share_response.signer_id,
            sig_share_response.signature_shares.clone(),
        );
```

**File:** src/state_machine/coordinator/fire.rs (L1113-1113)
```rust
        if message_nonce.sign_wait_signer_ids.is_empty() {
```

**File:** src/state_machine/coordinator/fire.rs (L1131-1135)
```rust
            let shares = message_nonce
                .public_nonces
                .iter()
                .flat_map(|(i, _)| self.signature_shares[i].clone())
                .collect::<Vec<SignatureShare>>();
```
