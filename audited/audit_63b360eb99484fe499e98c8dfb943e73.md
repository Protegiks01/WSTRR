### Title
Coordinator Panic Due to Premature Wait List Removal in Signature Share Validation

### Summary
The `gather_sig_shares()` function in FireCoordinator removes signers from the wait list before validating their signature shares, creating a state inconsistency that leads to a coordinator panic. A malicious signer can send an invalid SignatureShareResponse that passes initial checks but fails subsequent validation, causing their removal from the wait list without adding their shares to storage. When all other signers respond, the aggregation code attempts to access the missing shares using the bracket operator, resulting in a panic and coordinator crash.

### Finding Description

**Exact Code Location:**
`src/state_machine/coordinator/fire.rs`, function `gather_sig_shares()`, lines 1044 (premature wait list removal) and 1134 (panic trigger)

**Root Cause:**
The function removes a signer from `sign_wait_signer_ids` at line 1044 [1](#0-0)  before performing validation checks at lines 1046-1076 [2](#0-1) . If any validation check fails (e.g., `BadKeyIDsForSigner` at line 1075), the function returns an error, but the signer has already been removed from the wait list and their shares are never inserted into `signature_shares` [3](#0-2) .

This creates a critical inconsistency: the signer exists in `public_nonces` (from the nonce phase) but not in `signature_shares`. When aggregation triggers at line 1113 [4](#0-3) , the code at line 1134 uses the bracket operator to access `self.signature_shares[i]` for each signer in `public_nonces` [5](#0-4) . The bracket operator on `BTreeMap` panics when the key is not present, crashing the coordinator.

**Why Existing Mitigations Fail:**
- The wait list check at lines 1015-1025 [6](#0-5)  only prevents processing after removal, not the premature removal itself
- Error handling at lines 328-332 [7](#0-6)  catches validation errors but doesn't restore the wait list state
- Timeout mechanism at lines 178-186 [8](#0-7)  only marks signers still in the wait list as malicious, not those already removed
- The duplicate check at lines 1082-1085 [9](#0-8)  is defensive but doesn't prevent the state corruption from premature removal

**Comparison with FROST Implementation:**
The FrostCoordinator implementation correctly removes from the wait list AFTER validation and insertion at line 656 [10](#0-9) , preventing this vulnerability.

### Impact Explanation

**Specific Harm:**
A malicious or faulty signer can crash the coordinator node, causing denial of service for the signing round in progress and potentially disrupting transaction signing in systems using WSTS (e.g., Stacks blockchain).

**Quantified Impact:**
- Coordinator node crashes and must be restarted
- Current signing round fails completely
- All participants must restart the signing protocol
- In multi-coordinator setups, each coordinator instance can be individually crashed
- Repeated attacks can prevent signing operations from completing

**Who is Affected:**
- Coordinator node operators
- Users waiting for threshold signatures
- Systems depending on WSTS for transaction signing (e.g., Stacks miners)

**Severity Justification:**
This is **Low severity** per the audit scope definition: "Any remotely-exploitable denial of service in a node." The vulnerability allows a malicious signer to crash coordinator nodes but does not lead to invalid signatures, fund loss, or consensus failures.

### Likelihood Explanation

**Required Attacker Capabilities:**
- Must be a registered signer with assigned key_ids in the coordinator's configuration
- Must successfully participate in the DKG phase
- Must successfully participate in the nonce generation phase
- Must be selected for the signing committee for a specific message

**Attack Complexity:**
Low. The attacker simply needs to:
1. Participate normally through DKG and nonce phases
2. Send a `SignatureShareResponse` with invalid `key_ids` that don't match the configured key_ids
3. Wait for other signers to respond

**Economic Feasibility:**
Very feasible. No special resources required beyond being a protocol participant. The attack can be repeated for multiple signing rounds.

**Detection Risk:**
Medium. The validation error is logged and returned to the application, but the state corruption leading to the panic occurs later, potentially making root cause analysis difficult.

**Estimated Probability:**
High if the attacker is already a signer. Medium-to-High in systems where signer registration is permissioned but signers may be compromised or malicious.

### Recommendation

**Primary Fix:**
Move the wait list removal to occur AFTER all validation checks and share insertion, matching the FrostCoordinator pattern. Change the code order as follows:

1. Perform all validation checks (lines 1046-1076)
2. Check for duplicates (lines 1078-1086)
3. Insert shares (lines 1088-1091)
4. Remove from wait list (line 1044)

**Specific Code Changes:**
In `src/state_machine/coordinator/fire.rs`, function `gather_sig_shares()`:
- Move lines 1042-1044 to after line 1091
- This ensures signers are only removed from the wait list after their shares are successfully validated and stored

**Alternative Mitigation:**
If maintaining the current order is necessary for other reasons, use `.get()` instead of the bracket operator at line 1134:
- Change `self.signature_shares[i].clone()` to `self.signature_shares.get(i).ok_or(Error::MissingSignatureShare(i))?.clone()`
- This converts the panic into a proper error that can be handled

**Testing Recommendations:**
1. Add unit test that sends invalid key_ids after successful nonce phase
2. Verify coordinator returns error without panicking
3. Verify signer can retry with correct shares or is properly marked as malicious
4. Test timeout behavior with partially failed validation

**Deployment Considerations:**
This is a critical fix for production deployments. The change is low-risk as it only reorders operations without changing logic, matching the already-correct FrostCoordinator implementation.

### Proof of Concept

**Exploitation Steps:**

1. **Setup:** Coordinator configured with signers A, B, C. Signer A is malicious. Threshold requires 3/3 signers.

2. **DKG Phase:** All signers (A, B, C) complete DKG successfully.

3. **Nonce Phase:**
   - Coordinator sends `NonceRequest`
   - Signers A, B, C send valid `NonceResponse`
   - All signers added to `public_nonces` map
   - Coordinator transitions to `SigShareRequest` state

4. **Signature Share Phase - Attack:**
   - Coordinator sends `SignatureShareRequest`
   - **Signer A sends malicious `SignatureShareResponse`** with `key_ids` that don't match configured key_ids (e.g., sends `[999]` instead of configured `[1, 2]`)
   - Coordinator processes A's message in `gather_sig_shares()`:
     - Line 1019: A is in wait list, check passes
     - Line 1044: A removed from `sign_wait_signer_ids`
     - Line 1073: Validation detects key_ids mismatch
     - Line 1075: Returns `Error::BadKeyIDsForSigner(A)`
     - A NOT added to `signature_shares`
   - Coordinator returns error to application but remains in `SigShareGather` state
   
5. **Signature Share Phase - Continued:**
   - Signer B sends valid `SignatureShareResponse`
   - Added to `signature_shares`, removed from wait list
   - Signer C sends valid `SignatureShareResponse`
   - Added to `signature_shares`, removed from wait list
   - `sign_wait_signer_ids` is now empty

6. **Aggregation - Panic:**
   - Line 1113: Check `sign_wait_signer_ids.is_empty()` passes
   - Line 1131-1134: Build shares list by iterating `public_nonces`
   - Iteration reaches signer A (present in `public_nonces`)
   - Line 1134: `self.signature_shares[A]` attempts bracket access
   - **PANIC**: Key A not found in BTreeMap
   - Coordinator crashes

**Expected vs Actual Behavior:**
- **Expected:** Validation failures should either allow retry or mark signer as malicious without crashing
- **Actual:** Coordinator panics and crashes when aggregating shares

**Reproduction Instructions:**
Create a test with three signers, have one send invalid `key_ids` in their signature share response, then have the other two send valid responses. Observe the coordinator panic at aggregation time.

**Notes:**

Regarding the original audit question about duplicate handling at lines 1078-1086: The duplicate check itself is NOT a vulnerability. It is correct defensive programming that handles race conditions where two messages from the same signer might be processed concurrently. Returning `Ok()` for duplicates is the appropriate behavior since the signer has already been removed from the wait list and their shares are already stored.

The actual vulnerability is the premature wait list removal (line 1044) occurring before validation (lines 1046-1076), not the duplicate handling code that was questioned in the audit prompt.

### Citations

**File:** src/state_machine/coordinator/fire.rs (L178-186)
```rust
                            for signer_id in &self
                                .message_nonces
                                .get(&self.message)
                                .ok_or(Error::MissingMessageNonceInfo)?
                                .sign_wait_signer_ids
                            {
                                warn!("Mark signer {signer_id} as malicious");
                                self.malicious_signer_ids.insert(*signer_id);
                            }
```

**File:** src/state_machine/coordinator/fire.rs (L328-332)
```rust
                    if let Err(e) = self.gather_sig_shares(packet, signature_type) {
                        return Ok((
                            None,
                            Some(OperationResult::SignError(SignError::Coordinator(e))),
                        ));
```

**File:** src/state_machine/coordinator/fire.rs (L1015-1025)
```rust
        let waiting = response_info
            .sign_wait_signer_ids
            .contains(&sig_share_response.signer_id);

        if !waiting {
            warn!(
                "Sign round {} SignatureShareResponse for round {} from signer {} not in the wait list",
                self.current_sign_id, sig_share_response.sign_id, sig_share_response.signer_id,
            );
            return Ok(());
        }
```

**File:** src/state_machine/coordinator/fire.rs (L1042-1044)
```rust
        response_info
            .sign_wait_signer_ids
            .remove(&sig_share_response.signer_id);
```

**File:** src/state_machine/coordinator/fire.rs (L1046-1076)
```rust
        // check that the signer_id exists in the config
        let signer_public_keys = &self.config.public_keys.signers;
        if !signer_public_keys.contains_key(&sig_share_response.signer_id) {
            warn!(signer_id = %sig_share_response.signer_id, "No public key in config");
            return Err(Error::MissingPublicKeyForSigner(
                sig_share_response.signer_id,
            ));
        };

        // check that the key_ids match the config
        let Some(signer_key_ids) = self
            .config
            .public_keys
            .signer_key_ids
            .get(&sig_share_response.signer_id)
        else {
            warn!(signer_id = %sig_share_response.signer_id, "No keys IDs configured");
            return Err(Error::MissingKeyIDsForSigner(sig_share_response.signer_id));
        };

        let mut sig_share_response_key_ids = HashSet::new();
        for sig_share in &sig_share_response.signature_shares {
            for key_id in &sig_share.key_ids {
                sig_share_response_key_ids.insert(*key_id);
            }
        }

        if *signer_key_ids != sig_share_response_key_ids {
            warn!(signer_id = %sig_share_response.signer_id, "SignatureShareResponse key_ids didn't match config");
            return Err(Error::BadKeyIDsForSigner(sig_share_response.signer_id));
        }
```

**File:** src/state_machine/coordinator/fire.rs (L1082-1085)
```rust
        if have_shares {
            info!(signer_id = %sig_share_response.signer_id, "received duplicate SignatureShareResponse");
            // XXX should this be an error?  We should have already removed signer from wait set
            return Ok(());
```

**File:** src/state_machine/coordinator/fire.rs (L1088-1091)
```rust
        self.signature_shares.insert(
            sig_share_response.signer_id,
            sig_share_response.signature_shares.clone(),
        );
```

**File:** src/state_machine/coordinator/fire.rs (L1113-1113)
```rust
        if message_nonce.sign_wait_signer_ids.is_empty() {
```

**File:** src/state_machine/coordinator/fire.rs (L1131-1135)
```rust
            let shares = message_nonce
                .public_nonces
                .iter()
                .flat_map(|(i, _)| self.signature_shares[i].clone())
                .collect::<Vec<SignatureShare>>();
```

**File:** src/state_machine/coordinator/frost.rs (L652-656)
```rust
            self.signature_shares.insert(
                sig_share_response.signer_id,
                sig_share_response.signature_shares.clone(),
            );
            self.ids_to_await.remove(&sig_share_response.signer_id);
```
