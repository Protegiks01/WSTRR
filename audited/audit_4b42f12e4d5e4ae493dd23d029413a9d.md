### Title
Inconsistent Group Key Derivation Due to Unvalidated DkgEndBegin Signer IDs

### Summary
The `dkg_end_begin()` function accepts DkgEndBegin messages without validating that the `signer_ids` field matches the previously received `DkgPrivateBegin.signer_ids`. A malicious coordinator can exploit this by sending different `DkgEndBegin.signer_ids` lists to different signers, causing each signer to compute a different group public key. This breaks the threshold signature scheme and can lead to network partition or complete signing failure.

### Finding Description

**Exact Code Location:** [1](#0-0) 

**Root Cause:**
The `dkg_end_begin()` function simply stores the received `DkgEndBegin` message without any validation: [1](#0-0) 

The function does not check whether `dkg_end_begin.signer_ids` is consistent with the `dkg_private_begin.signer_ids` that was previously cached. This allows arbitrary signer ID lists to be accepted.

**How the Vulnerability Manifests:**

During `dkg_private_begin()`, signers determine which key_ids are "active" based on `DkgPrivateBegin.signer_ids` and only encrypt private shares for those active key_ids: [2](#0-1) 

Later, when `dkg_ended()` executes, it uses `DkgEndBegin.signer_ids` to determine which polynomial commitments to include in the group key calculation: [3](#0-2) 

The commitments from only these signers are inserted and used: [4](#0-3) 

Finally, `compute_secrets()` is called with this filtered set of commitments: [5](#0-4) 

The group key is computed by summing the constant terms of the polynomial commitments: [6](#0-5) 

**Why Existing Mitigations Fail:**

1. **Packet Signature Verification:** While signers verify packet signatures, this only authenticates that the message came from the coordinator. It does not prevent the coordinator from sending different messages to different signers: [7](#0-6) 

2. **DkgEnd Messages:** The `DkgEnd` message structure does not include the computed group key, preventing signers from detecting inconsistencies: [8](#0-7) 

3. **No Cross-Signer Validation:** Signers do not communicate with each other to verify they received the same `DkgEndBegin.signer_ids` or computed the same group key.

### Impact Explanation

**Specific Harm:**
A malicious coordinator can cause different signers to derive different group public keys. For example:
- Signer 0 receives `DkgEndBegin` with `signer_ids = [0, 1, 2]` and computes `group_key_0 = P₀(0) + P₁(0) + P₂(0)`
- Signers 1 and 2 receive `DkgEndBegin` with `signer_ids = [0, 1]` and compute `group_key_1 = P₀(0) + P₁(0)`

**Quantified Impact:**
- **Protocol Failure:** Any signing operation will fail because signers use different group keys to compute signature shares. The shares cannot be aggregated into a valid signature.
- **Network Partition:** In a distributed system where different nodes use different signers, this creates an unintended split in protocol state. Nodes with different signers will have incompatible views of the group key.
- **Denial of Service:** The threshold signature system becomes completely non-functional, preventing any transactions from being signed.

**Who is Affected:**
- All signers in the DKG round receive inconsistent state
- Any system depending on WSTS for threshold signatures (e.g., Stacks blockchain) experiences signing failure
- The entire network partition is affected if signers are distributed across nodes

**Severity Justification:**
This maps to **High** severity under the protocol scope: "Any unintended chain split or network partition." The inconsistent group keys across signers create incompatible protocol states that can partition the network. While it may not directly confirm invalid transactions, it creates conditions where different nodes process signatures differently, potentially leading to consensus failures.

### Likelihood Explanation

**Required Attacker Capabilities:**
- Control of the coordinator node
- Ability to send different messages to different signers (requires breaking broadcast assumption)
- Network-level access to intercept or selectively deliver messages

**Attack Complexity:**
- **Low Complexity:** Once the coordinator is compromised, the attack is trivial to execute. The attacker simply constructs different `DkgEndBegin` packets with different `signer_ids` lists and sends them to different signers.
- **No Cryptographic Breaks Required:** The attack exploits missing validation logic, not cryptographic weaknesses.

**Economic Feasibility:**
- Coordinator compromise is a realistic threat in production systems
- The attack requires no special resources beyond network access
- Can be executed in a single DKG round

**Detection Risk:**
- **Low Detection:** Signers report `DkgStatus::Success` because they successfully compute a group key (just not the same one)
- The inconsistency only becomes apparent when attempting to aggregate signatures
- No immediate error or alert is triggered during DKG

**Estimated Probability:**
- **Moderate to High:** The coordinator is a semi-trusted component in threshold signature systems, and compromise scenarios are realistic
- The FIRE coordinator variant already sends different `signer_ids` lists based on which signers respond, making this behavior expected in some contexts

### Recommendation

**Primary Fix - Add Validation in `dkg_end_begin()`:**

Add validation to ensure `DkgEndBegin.signer_ids` is a subset of (or equal to) the previously received `DkgPrivateBegin.signer_ids`: [1](#0-0) 

Proposed changes:
```rust
pub fn dkg_end_begin(&mut self, dkg_end_begin: &DkgEndBegin) -> Result<Vec<Message>, Error> {
    // Validate signer_ids consistency
    if let Some(dkg_private_begin) = &self.dkg_private_begin_msg {
        let private_begin_set: HashSet<u32> = dkg_private_begin.signer_ids.iter().copied().collect();
        for signer_id in &dkg_end_begin.signer_ids {
            if !private_begin_set.contains(signer_id) {
                warn!(
                    "DkgEndBegin contains signer_id {} not in DkgPrivateBegin",
                    signer_id
                );
                return Err(Error::BadStateChange(
                    "DkgEndBegin signer_ids must be subset of DkgPrivateBegin signer_ids".into()
                ));
            }
        }
    } else {
        return Err(Error::BadStateChange(
            "Received DkgEndBegin without DkgPrivateBegin".into()
        ));
    }
    
    self.dkg_end_begin_msg = Some(dkg_end_begin.clone());
    // ... rest of function
}
```

**Alternative Mitigation - Include Group Key in DkgEnd:**

Modify the `DkgEnd` message structure to include the computed group key, allowing the coordinator to detect inconsistencies: [8](#0-7) 

This would allow the coordinator to verify all signers computed the same group key.

**Testing Recommendations:**
1. Add unit test with different `DkgPrivateBegin` and `DkgEndBegin` signer_ids
2. Add integration test simulating malicious coordinator sending different messages
3. Verify error handling and state machine behavior on validation failure
4. Test FIRE coordinator behavior where signer_ids legitimately differ

**Deployment Considerations:**
- Breaking change that may affect existing coordinator implementations
- Requires coordination between signer and coordinator updates
- FIRE coordinator logic must be reviewed to ensure it constructs valid signer_ids lists

### Proof of Concept

**Exploitation Steps:**

1. **Setup:** Configure 3 signers (IDs 0, 1, 2) with threshold requiring all 3 for DKG

2. **DKG Public Phase:** Coordinator broadcasts `DkgBegin` with `dkg_id = 1`. All signers send `DkgPublicShares`.

3. **DKG Private Phase:** Coordinator sends `DkgPrivateBegin` to all signers:
   ```
   DkgPrivateBegin {
       dkg_id: 1,
       signer_ids: [0, 1, 2],
       key_ids: [0, 1, 2]
   }
   ```
   All signers encrypt and send private shares for all three signers' key_ids.

4. **Malicious DKG End Phase:** Coordinator sends different `DkgEndBegin` messages:
   - To Signer 0:
     ```
     DkgEndBegin {
         dkg_id: 1,
         signer_ids: [0, 1, 2],  // All three signers
         key_ids: [0, 1, 2]
     }
     ```
   - To Signers 1 and 2:
     ```
     DkgEndBegin {
         dkg_id: 1,
         signer_ids: [0, 1],     // Only two signers
         key_ids: [0, 1]
     }
     ```

5. **Group Key Computation:**
   - Signer 0 computes: `group_key_0 = poly_0[0] + poly_1[0] + poly_2[0]`
   - Signer 1 computes: `group_key_1 = poly_0[0] + poly_1[0]`
   - Signer 2 computes: `group_key_2 = poly_0[0] + poly_1[0]`

6. **Result Verification:**
   - All signers report `DkgStatus::Success` to coordinator
   - Signer 0 has `group_key_0 ≠ group_key_1 = group_key_2`
   - Protocol appears successful but is in inconsistent state

7. **Signing Failure:** When coordinator requests signatures:
   - Signers compute signature shares using their different group keys
   - Shares cannot be aggregated into valid signature
   - Signature verification fails

**Expected vs Actual Behavior:**
- **Expected:** All signers should compute the same group key based on consistent signer participation
- **Actual:** Signers compute different group keys based on the `DkgEndBegin.signer_ids` they received, with no validation or detection

**Reproduction Instructions:**
1. Modify test harness to send different `DkgEndBegin` messages to different signers
2. Observe that `dkg_end_begin()` accepts all messages without error
3. Verify signers compute different group keys by inspecting their internal state
4. Attempt signing and observe aggregation failure

### Citations

**File:** src/state_machine/signer/mod.rs (L463-470)
```rust
        if self.verify_packet_sigs {
            let Some(coordinator_public_key) = self.coordinator_public_key else {
                return Err(Error::MissingCoordinatorPublicKey);
            };
            if !packet.verify(&self.public_keys, &coordinator_public_key) {
                return Err(Error::InvalidPacketSignature);
            }
        }
```

**File:** src/state_machine/signer/mod.rs (L529-534)
```rust
        let signer_ids_set: HashSet<u32> = dkg_end_begin
            .signer_ids
            .iter()
            .filter(|&&id| id < self.total_signers)
            .copied()
            .collect::<HashSet<u32>>();
```

**File:** src/state_machine/signer/mod.rs (L551-562)
```rust
        for signer_id in &signer_ids_set {
            if let Some(shares) = self.dkg_public_shares.get(signer_id) {
                if shares.comms.is_empty() {
                    missing_public_shares.insert(*signer_id);
                } else {
                    for (party_id, comm) in shares.comms.iter() {
                        if !check_public_shares(comm, threshold, &self.dkg_id.to_be_bytes()) {
                            bad_public_shares.insert(*signer_id);
                        } else {
                            self.commitments.insert(*party_id, comm.clone());
                        }
                    }
```

**File:** src/state_machine/signer/mod.rs (L612-616)
```rust
            match self.signer.compute_secrets(
                &self.decrypted_shares,
                &self.commitments,
                &self.dkg_id.to_be_bytes(),
            ) {
```

**File:** src/state_machine/signer/mod.rs (L903-910)
```rust
        let mut active_key_ids = HashSet::new();
        for signer_id in &dkg_private_begin.signer_ids {
            if let Some(key_ids) = self.public_keys.signer_key_ids.get(signer_id) {
                for key_id in key_ids {
                    active_key_ids.insert(*key_id);
                }
            }
        }
```

**File:** src/state_machine/signer/mod.rs (L959-971)
```rust
    pub fn dkg_end_begin(&mut self, dkg_end_begin: &DkgEndBegin) -> Result<Vec<Message>, Error> {
        let msgs = vec![];

        self.dkg_end_begin_msg = Some(dkg_end_begin.clone());

        info!(
            signer_id = %self.signer_id,
            dkg_id = %self.dkg_id,
            "received DkgEndBegin"
        );

        Ok(msgs)
    }
```

**File:** src/v2.rs (L135-140)
```rust
        for (i, comm) in public_shares.iter() {
            if !check_public_shares(comm, threshold, ctx) {
                bad_ids.push(*i);
            } else {
                self.group_key += comm.poly[0];
            }
```

**File:** src/net.rs (L245-252)
```rust
pub struct DkgEnd {
    /// DKG round ID
    pub dkg_id: u64,
    /// Signer ID
    pub signer_id: u32,
    /// DKG status for this Signer after receiving public/private shares
    pub status: DkgStatus,
}
```
