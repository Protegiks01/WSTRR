### Title
Inconsistent Timeout Configuration Enables Targeted Denial of Service in FIRE Coordinator

### Summary
The FIRE coordinator's timeout mechanism allows mixing Some and None timeout values across different protocol phases without validation, creating an exploitable inconsistency. A malicious or crashed signer can strategically respond during phases with configured timeouts but remain silent during phases without timeouts, causing the coordinator to hang indefinitely in untimed states and preventing DKG or signing operations from completing.

### Finding Description

**Code Location:**
- Config struct definition: [1](#0-0) 
- Config constructor allowing arbitrary timeout mixing: [2](#0-1) 
- Timeout checking logic: [3](#0-2) 
- Process method that checks timeouts before processing packets: [4](#0-3) 

**Root Cause:**
The Config struct defines five independent timeout fields as `Option<Duration>`, allowing each to be independently set to Some or None. [5](#0-4)  The `Config::with_timeouts()` constructor accepts these values without any validation to ensure consistency. [2](#0-1) 

The FIRE coordinator's `process_timeout()` method uses a double-optional pattern that only fires timeouts when both the start time AND the timeout config are Some: [6](#0-5)  This pattern is repeated for all five phases (DkgPublicGather, DkgPrivateGather, DkgEndGather, NonceGather, SigShareGather).

When a timeout is configured as None, the timeout check is completely skipped for that state, allowing the coordinator to wait indefinitely. The WSTS specification explicitly states that timeouts are necessary: "We must set a timeout for the signing portion of the session, or else byzantine actors can slow the protocol indefinitely" and "without them we will never be able to declare that a round is over, since a byzantine set of actors can simply stall any session." [7](#0-6) 

**Why Existing Mitigations Fail:**
No validation exists to prevent mixing Some/None configurations. The coordinator cannot detect or warn about this misconfiguration. Tests only use all-Some or all-None configurations, never testing the mixed case. [8](#0-7) 

### Impact Explanation

**Specific Harm:**
When timeouts are inconsistently configured (e.g., `dkg_public_timeout=Some(10s)`, `dkg_private_timeout=Some(10s)`, `dkg_end_timeout=None`), a malicious or crashed signer can:
1. Respond normally during DkgPublicGather and DkgPrivateGather phases (which have timeouts)
2. Never send DkgEnd messages
3. Cause the coordinator to hang indefinitely in DkgEndGather state [9](#0-8) 

Similarly for signing: if `nonce_timeout=Some` but `sign_timeout=None`, a signer can provide nonces but never provide signature shares, causing an indefinite hang in SigShareGather. [10](#0-9) 

**Quantified Impact:**
- Complete DOS of DKG operations prevents establishing new group keys
- Complete DOS of signing operations prevents accessing funds controlled by the threshold key
- No error indication or recovery mechanism (silent failure)
- Affects all parties waiting on that coordinator instance

**Affected Parties:**
All participants in the DKG or signing round coordinated by the misconfigured FIRE coordinator.

**Severity Justification:**
This maps to **Medium severity** ("transient consensus failures") because:
- It prevents signing operations from completing, blocking transaction confirmations
- The coordinator can be restarted with proper configuration to recover
- It does not cause direct fund loss, only temporary inaccessibility
- It requires either configuration error or a malicious/crashed signer

### Likelihood Explanation

**Required Attacker Capabilities:**
- Must be a registered signer with valid signer_id in the coordinator's config
- Coordinator must have inconsistent timeout configuration (Some timeouts but not all)
- No cryptographic breaks required

**Attack Complexity:**
LOW - The attacker simply:
1. Responds normally in phases with configured timeouts
2. Remains silent (crashes or refuses to respond) in phases without timeouts
3. The coordinator automatically hangs in the untimed state

**Economic Feasibility:**
HIGH - No resources required beyond signer participation. The attack is passive (non-response).

**Detection Risk:**
LOW - The behavior appears identical to a crashed or offline signer. No malicious activity is detectable since the attack is simply non-responsiveness in specific phases.

**Probability of Success:**
- Configuration error by operator: MEDIUM-HIGH (easy to misconfigure when setting up timeouts for "critical" phases only)
- Malicious exploitation given misconfiguration: HIGH (100% success rate if configuration exists)
- Overall: MEDIUM (depends on configuration being present)

### Recommendation

**Primary Fix:**
Add validation to `Config::with_timeouts()` to enforce all-or-nothing timeout configuration:

```rust
pub fn with_timeouts(...) -> Result<Self, ConfigError> {
    let timeouts = vec![
        dkg_public_timeout, dkg_private_timeout, dkg_end_timeout,
        nonce_timeout, sign_timeout
    ];
    
    let some_count = timeouts.iter().filter(|t| t.is_some()).count();
    if some_count != 0 && some_count != timeouts.len() {
        return Err(ConfigError::InconsistentTimeouts(
            "All timeouts must be configured together or all must be None"
        ));
    }
    
    // ... rest of constructor
}
```

**Alternative Mitigations:**
1. Set a global default timeout for any phase where specific timeout is None
2. Add runtime warnings when detecting None timeouts during coordinator initialization
3. Document the requirement clearly that all timeouts must be configured consistently

**Testing Recommendations:**
1. Add test cases for mixed Some/None timeout configurations expecting validation errors
2. Test DKG hanging scenarios with partial timeout configurations
3. Test signing hanging scenarios with partial timeout configurations

**Deployment Considerations:**
- Existing deployed coordinators with inconsistent timeouts will fail validation
- Provide clear migration guide for operators to fix their configurations
- Consider backward-compatible option with warnings before enforcing strict validation

### Proof of Concept

**Exploitation Steps:**

1. **Setup Phase:**
   Configure coordinator with:
   ```
   dkg_public_timeout: Some(Duration::from_secs(10))
   dkg_private_timeout: Some(Duration::from_secs(10))
   dkg_end_timeout: None
   nonce_timeout: Some(Duration::from_secs(10))
   sign_timeout: None
   ```

2. **DKG Attack:**
   - Start DKG round
   - Attacker responds with DkgPublicShares within timeout
   - Attacker responds with DkgPrivateShares within timeout
   - Attacker never sends DkgEnd message
   - Coordinator enters DkgEndGather state at [11](#0-10) 
   - Timeout check skips because `dkg_end_timeout` is None [9](#0-8) 
   - Coordinator hangs indefinitely waiting for DkgEnd

3. **Signing Attack:**
   - Start signing round
   - Attacker responds with NonceResponse within timeout
   - Coordinator moves to SigShareGather at [12](#0-11) 
   - Attacker never sends SignatureShareResponse
   - Timeout check skips because `sign_timeout` is None [10](#0-9) 
   - Coordinator hangs indefinitely waiting for signature shares

**Expected vs Actual Behavior:**
- Expected: Protocol should timeout and fail gracefully in all phases or wait indefinitely in all phases (consistent behavior)
- Actual: Protocol times out in some phases but hangs forever in others (inconsistent behavior exploitable for targeted DOS)

**Reproduction:**
Use the test setup from [13](#0-12)  but with mixed timeout configuration, then have one signer fail to respond in a phase without a configured timeout.

### Citations

**File:** src/state_machine/coordinator/mod.rs (L132-158)
```rust
#[derive(Default, Clone, PartialEq)]
pub struct Config {
    /// total number of signers
    pub num_signers: u32,
    /// total number of keys
    pub num_keys: u32,
    /// threshold of keys needed to form a valid signature
    pub threshold: u32,
    /// threshold of keys needed to complete DKG (must be >= threshold)
    pub dkg_threshold: u32,
    /// private key used to sign network messages
    pub message_private_key: Scalar,
    /// timeout to gather DkgPublicShares messages
    pub dkg_public_timeout: Option<Duration>,
    /// timeout to gather DkgPrivateShares messages
    pub dkg_private_timeout: Option<Duration>,
    /// timeout to gather DkgEnd messages
    pub dkg_end_timeout: Option<Duration>,
    /// timeout to gather nonces
    pub nonce_timeout: Option<Duration>,
    /// timeout to gather signature shares
    pub sign_timeout: Option<Duration>,
    /// the public keys and key_ids for all signers
    pub public_keys: PublicKeys,
    /// whether to verify the signature on Packets
    pub verify_packet_sigs: bool,
}
```

**File:** src/state_machine/coordinator/mod.rs (L202-232)
```rust
    #[allow(clippy::too_many_arguments)]
    /// Create a new config object with the passed timeouts
    pub fn with_timeouts(
        num_signers: u32,
        num_keys: u32,
        threshold: u32,
        dkg_threshold: u32,
        message_private_key: Scalar,
        dkg_public_timeout: Option<Duration>,
        dkg_private_timeout: Option<Duration>,
        dkg_end_timeout: Option<Duration>,
        nonce_timeout: Option<Duration>,
        sign_timeout: Option<Duration>,
        public_keys: PublicKeys,
    ) -> Self {
        Config {
            num_signers,
            num_keys,
            threshold,
            dkg_threshold,
            message_private_key,
            dkg_public_timeout,
            dkg_private_timeout,
            dkg_end_timeout,
            nonce_timeout,
            sign_timeout,
            public_keys,
            verify_packet_sigs: true,
        }
    }
}
```

**File:** src/state_machine/coordinator/mod.rs (L563-571)
```rust
    pub fn setup_with_timeouts<Coordinator: CoordinatorTrait, SignerType: SignerTrait>(
        num_signers: u32,
        keys_per_signer: u32,
        dkg_public_timeout: Option<Duration>,
        dkg_private_timeout: Option<Duration>,
        dkg_end_timeout: Option<Duration>,
        nonce_timeout: Option<Duration>,
        sign_timeout: Option<Duration>,
    ) -> (Vec<Coordinator>, Vec<Signer<SignerType>>) {
```

**File:** src/state_machine/coordinator/fire.rs (L72-211)
```rust
    pub fn process_timeout(&mut self) -> Result<(Option<Packet>, Option<OperationResult>), Error> {
        let now = Instant::now();
        match self.state.clone() {
            State::Idle => {}
            State::DkgPublicDistribute => {}
            State::DkgPublicGather => {
                if let Some(start) = self.dkg_public_start {
                    if let Some(timeout) = self.config.dkg_public_timeout {
                        if now.duration_since(start) > timeout {
                            // check dkg_threshold to determine if we can continue
                            let dkg_size = self.compute_dkg_public_size()?;

                            if self.config.dkg_threshold > dkg_size {
                                error!("Timeout gathering DkgPublicShares for dkg round {} signing round {} iteration {}, dkg_threshold not met ({dkg_size}/{}), unable to continue", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                let wait = self.dkg_wait_signer_ids.iter().copied().collect();
                                return Ok((
                                    None,
                                    Some(OperationResult::DkgError(DkgError::DkgPublicTimeout(
                                        wait,
                                    ))),
                                ));
                            } else {
                                // we hit the timeout but met the threshold, continue
                                warn!("Timeout gathering DkgPublicShares for dkg round {} signing round {} iteration {}, dkg_threshold was met ({dkg_size}/{}), ", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                self.public_shares_gathered()?;
                                let packet = self.start_private_shares()?;
                                return Ok((Some(packet), None));
                            }
                        }
                    }
                }
            }
            State::DkgPrivateDistribute => {}
            State::DkgPrivateGather => {
                if let Some(start) = self.dkg_private_start {
                    if let Some(timeout) = self.config.dkg_private_timeout {
                        if now.duration_since(start) > timeout {
                            // check dkg_threshold to determine if we can continue
                            let dkg_size = self.compute_dkg_private_size()?;

                            if self.config.dkg_threshold > dkg_size {
                                error!("Timeout gathering DkgPrivateShares for dkg round {} signing round {} iteration {}, dkg_threshold not met ({dkg_size}/{}), unable to continue", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                let wait = self.dkg_wait_signer_ids.iter().copied().collect();
                                return Ok((
                                    None,
                                    Some(OperationResult::DkgError(DkgError::DkgPrivateTimeout(
                                        wait,
                                    ))),
                                ));
                            } else {
                                // we hit the timeout but met the threshold, continue
                                warn!("Timeout gathering DkgPrivateShares for dkg round {} signing round {} iteration {}, dkg_threshold was met ({dkg_size}/{}), ", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id, self.config.dkg_threshold);
                                self.private_shares_gathered()?;
                                let packet = self.start_dkg_end()?;
                                return Ok((Some(packet), None));
                            }
                        }
                    }
                }
            }
            State::DkgEndDistribute => {}
            State::DkgEndGather => {
                if let Some(start) = self.dkg_end_start {
                    if let Some(timeout) = self.config.dkg_end_timeout {
                        if now.duration_since(start) > timeout {
                            error!("Timeout gathering DkgEnd for dkg round {} signing round {} iteration {}, unable to continue", self.current_dkg_id, self.current_sign_id, self.current_sign_iter_id);
                            let wait = self.dkg_wait_signer_ids.iter().copied().collect();
                            return Ok((
                                None,
                                Some(OperationResult::DkgError(DkgError::DkgEndTimeout(wait))),
                            ));
                        }
                    }
                }
            }
            State::NonceRequest(_signature_type) => {}
            State::SigShareRequest(_signature_type) => {}
            State::NonceGather(_signature_type) => {
                if let Some(start) = self.nonce_start {
                    if let Some(timeout) = self.config.nonce_timeout {
                        if now.duration_since(start) > timeout {
                            error!("Timeout gathering nonces for signing round {} iteration {}, unable to continue", self.current_sign_id, self.current_sign_iter_id);
                            let recv = self
                                .message_nonces
                                .get(&self.message)
                                .ok_or(Error::MissingMessageNonceInfo)?
                                .sign_wait_signer_ids
                                .iter()
                                .copied()
                                .collect();
                            let mal = self.malicious_signer_ids.iter().copied().collect();
                            return Ok((
                                None,
                                Some(OperationResult::SignError(SignError::NonceTimeout(
                                    recv, mal,
                                ))),
                            ));
                        }
                    }
                }
            }
            State::SigShareGather(signature_type) => {
                if let Some(start) = self.sign_start {
                    if let Some(timeout) = self.config.sign_timeout {
                        if now.duration_since(start) > timeout {
                            warn!("Timeout gathering signature shares for signing round {} iteration {}", self.current_sign_id, self.current_sign_iter_id);
                            for signer_id in &self
                                .message_nonces
                                .get(&self.message)
                                .ok_or(Error::MissingMessageNonceInfo)?
                                .sign_wait_signer_ids
                            {
                                warn!("Mark signer {signer_id} as malicious");
                                self.malicious_signer_ids.insert(*signer_id);
                            }

                            let num_malicious_keys: u32 =
                                self.compute_num_key_ids(self.malicious_signer_ids.iter())?;

                            if self.config.num_keys - num_malicious_keys < self.config.threshold {
                                error!("Insufficient non-malicious signers, unable to continue");
                                let mal = self.malicious_signer_ids.iter().copied().collect();
                                return Ok((
                                    None,
                                    Some(OperationResult::SignError(
                                        SignError::InsufficientSigners(mal),
                                    )),
                                ));
                            }

                            self.move_to(State::NonceRequest(signature_type))?;
                            let packet = self.request_nonces(signature_type)?;
                            return Ok((Some(packet), None));
                        }
                    }
                }
            }
        }
        Ok((None, None))
    }
```

**File:** src/state_machine/coordinator/fire.rs (L472-474)
```rust
        self.move_to(State::DkgEndGather)?;
        self.dkg_end_start = Some(Instant::now());
        Ok(dkg_end_begin_msg)
```

**File:** src/state_machine/coordinator/fire.rs (L992-993)
```rust
        self.move_to(State::SigShareGather(signature_type))?;
        self.sign_start = Some(Instant::now());
```

**File:** src/state_machine/coordinator/fire.rs (L1441-1454)
```rust
    /// Process the timeouts, and if none of them fire then process the passed packet
    /// If a timeout does fire, then the coordinator state has changed; this means the
    /// packet is now stale and must be dropped
    fn process(
        &mut self,
        packet: &Packet,
    ) -> Result<(Option<Packet>, Option<OperationResult>), Error> {
        let (outbound_packet, operation_result) = self.process_timeout()?;
        if outbound_packet.is_some() || operation_result.is_some() {
            return Ok((outbound_packet, operation_result));
        }

        self.process_message(packet)
    }
```

**File:** src/state_machine/coordinator/fire.rs (L2084-2092)
```rust
            setup_with_timeouts::<FireCoordinator<Aggregator>, SignerType>(
                num_signers,
                keys_per_signer,
                Some(timeout),
                Some(timeout),
                Some(timeout),
                Some(timeout),
                Some(timeout),
            );
```

**File:** wsts.tex (L452-477)
```tex
So when round $i$ begins, we request nonces for session $j = 0$ from $A$.  Once we get $k = T$ nonces for $i,j$, the corresponding parties will be selected for the signing set $S_j$.  We then request signature shares from all parties in $S_j$.  If all sign correctly, and we obtain a valid signature, round $i$ is complete.  We must set a timeout for the signing portion of the session, or else byzantine actors can slow the protocol indefinitely.

Parties who do not return signatures, or return invalid signatures, are removed from $A$.  Parties who did not return a nonce for round $j$ are also removed.  We then we begin session $j+1$.  All members of $A$ are then requested to give nonces for session $j+1$.  The same process happens as before; when we get $k = T$ nonces we form $S_{j+1}$ and again request signature shares.

As before, if the aggregate signature is valid, round $i$ is complete.  Otherwise, we begin a new session $j+2$, and continue in the same vein.  Since each session will either complete or remove some parties from $A$, this algorithm will eventually terminate.  When $|A| < T$, signing round $i$ has failed.

When running a FIRE round with WSTS vs FROST, the only difference is that we must count the number of keys controlled by each party who responds with a nonce in every session.  Only when the sum of keys controlled by the given nonces equals or exceeds $T$ do we form $S_j$ and begin the signing portion of the session.  


\subsection{
  ROAST
}

ROAST\cite{roast} is a wrapper around FROST and other threshold signature schemes which provides robustness and asynchronicity.  It operates in a very similar manner to FIRE, but the asynchonicity allows for faster completion if there exists a set of honest and responsive signers.

ROAST differs from FIRE by allowing session $j+1$ to run in parallel with session $j$.  To do so, it is necessary to keep track of two sets: $R$ is the set of responsive signers, and $M$ of known malicious signers.  Once a party becomes a member of set $M$, all messages from that party will be ignored, and if the governance model includes sanctions on bad actors, those in $M$ will be nominated for such.

The ROAST paper describes the protocol in terms of events and responses, which makes an initial read somewhat challenging.  We will describe it here more linearly.  So as with FIRE, signing round $i$ begins session $j = 0$ with the coordinator asking all signers for nonces.  $R$ and $M$ are initially the null set.  Signers who receive a request to begin $i,0$ respond with a nonce $n_0$ only; this will be different $\forall j \neq 0$.

As the coordinator receives responses, the responding signers are placed into $R$, and their nonces are placed in $\rho_j$.  Once $|R| = T$, the coordinator sends $(\rho_j, R)$ to the signers to request signature shares.  Signers respond with signature $z_j$, and also a nonce $n_{j+1}$.  Piggybacking the next session's nonce with the current session's signature share improves the responsiveness of the protocol.

Crucially, when the coordinator sends $(\rho_j, R)$ to signers, those signers in $R$ are removed.  As they respond, the signers are placed back into $R$ if their signature shares are valid; if they are not valid, those signers are placed into $M$ and declared malicious.  Signers who respond with nonces for the current round while the signing session continues are placed into $R$ with those who provided valid signatures.

As signature shares and nonces are received, the coordinator does two things.  It first checks to see if it has received a full set of valid signature shares; if this happens, and the group signature is valid, then round $i$ is complete.  At the same time, the size of $R$ is examined.  When $|R| = T$, session $j+1$ begins.  Crucially, this happens in parallel to session $j$.

As the sessions continue for each signing round, it is clear that if there exists a threshold set of honest and responsive signers, the protocol will complete succesfully.  Every session will either succeed or add malicious actors to $M$.  If $|M| > (N-T)$, then the signing round has failed.  Likewise, we will never have more than $N-T$ sessions for any round.  Though the paper does not describe the use of timeouts, without them we will never be able to declare that a round is over, since a byzantine set of actors can simply stall any session where one of the set is a participant.  But since the session timeouts run in parallel, the speedup even in the byzantine failure case is significant.
```
